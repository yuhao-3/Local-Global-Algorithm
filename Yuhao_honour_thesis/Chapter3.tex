\chapter{Method}
\label{Chapter3}
\section{Introduction}
The main idea behind the method introduced in this study is to refine the MFVB posterior parameter estimate for the regression coefficients $\beta$ (represented by $\tilde{\mu}$ and $\tilde{\Sigma}$). By assuming a mean field variational family $q(\theta) = \prod_i q(\theta_i)$ and using Gaussian approximation, denoted as $q^*(\theta) \sim N(\tilde{\mu},\tilde{\Sigma})$, the aim is to approximate the global Bayesian Lasso posterior $p(\theta|\mathcal{D})$ while incorporating local parameter information. It is observed that the marginal likelihood approximately follows a lasso distribution when a Laplace prior is assigned. Consequently, the objective is to determine the mathematical expression of the local mean parameter $\mu_j^{*}$ and local variance $\Sigma_{jj}^{*}$ from the Lasso distribution at a local level, as well as the global mean $\tilde{\mu}$ and global variance $\tilde{\Sigma}$ of $q^*(\theta)$ for Gaussian approximation. This can be achieved through iterative updates that correct the global parameter estimate based on the local parameter expression for each $\beta_j$.

\section{Basic Setting for the Bayesian Lasso Problem}
Based on MFVB approach, we can approximate the posterior distribution as follows:
\begin{equation}
	\label{eq:assum}
	p(\beta,\sigma^2|\mathcal{D})\approx q(\beta,\sigma^2) = q(\beta)q(\sigma^2).
\end{equation}
We can divide up the set of parameters of interest $\theta = {\beta_1,\ldots,\beta_p, \sigma^2}$ into two parts
$\theta_1$: $\beta_j$ current variable and $\theta_{2}: \beta_{-j}$ , other variables.
The marginal log-likelihood of $\theta_1$ can be divided up into the ELBO part and the KL divergence part as follows:
\begin{equation}
	\label{LocalGlobalVI}
	\log(\mathcal{D},\theta_1) = \mathbb{E}_{q(\theta_{2}|\theta_1)}\left[ \log\left(\frac{p(\mathcal{D},\theta_1,\theta_{2})}{q(\theta_{2}|\theta_1)} \right)\right] + \text{KL}(q(\theta_{2}|\theta_1),p(\theta_{2}|\mathcal{D},\theta_1)).
\end{equation}
Since the KL divergence is greater than 0, the marginal log-likelihood of $\theta_1$ and $\mathcal{D}$ has a more tractable ELBO: 
\begin{equation}
	\log(\mathcal{D},\theta_1) \geq \mathbb{E}_{q(\theta_{2}|\theta_1)}\left[\log\left(\frac{p(\mathcal{D},\theta_1,\theta_{2})}{q(\theta_{2}|\theta_1)} \right)\right].
\end{equation}
When $q(\theta_{2}|\theta_1) = p(\theta_{2}|\mathcal{D},\theta_1)$ then 
$$
\log(\mathcal{D},\theta_1) = \mathbb{E}_{p(\theta_{2}|\mathcal{D},\theta_1)}[\log p(\mathcal{D},\theta_1,\theta_{2})].
$$
In Bayesian Lasso $\theta = (\beta,\sigma^2)$, however, in this study we do not discuss the update of $\sigma^2$. Thus,
the conditional distribution $q(\beta_{-j}|\beta_{j})$ for any $j$th variable can be derived by $q(\beta_{-j}|\beta_{j}) \propto q(\mathbf{\beta})$, resulting another multivariate normal distribution with dimension of $p-1$ as shown in \autoref{eq:condNormal}
\begin{equation}
	\label{eq:condNormal}
	q(\beta_{-j}|\beta_{j}) = N_{p-1}(\mu_{-j}+\Sigma_{-j,j}\Sigma_{j,j}^{-1}(\beta_j-\mu_j), \Sigma_{-j,j} \Sigma_{-j,-j}^{-1}\Sigma_{j,j}).
\end{equation}
Based on \autoref{eq:assum}, \autoref{eq:condNormal}, and  \autoref{eq:assum}, the result from \autoref{eq:condNormal}, the estimated marginal log likelihood for each $\beta_j$ after taking expectation with respect to $q(\theta)$ is:
\begin{equation}
	\label{eq:MarLike}
	\begin{aligned}
		\log p(\mathcal{D},\beta_j) &= \mathbb{E}_{\beta_{-j},\sigma^2|\mathcal{D},\beta_j} 	\log(p(\beta_j|\mathcal{D},\beta_{-j},\sigma^2))\\
		& \approx \mathbb{E}_{q(\beta_{-j}|\beta_j)q(\sigma^2)}
		\log(p(\beta_j|\mathcal{D},\beta_{-j},\sigma^2))\\
		& \propto \mathbb{E}_{q(\beta_{-j}|\beta_j)q(\sigma^2)}\left[-\frac{||X_j||_2^2}{2\sigma^2}\beta_j^2 + \frac{X_j^T(y - X_{-j}\beta_{-j})}{\sigma^2}\beta_j - \frac{\lambda}{\sigma}|\beta_j|\right]\\
		&= \frac{\tilde{a}}{\tilde{b}}(y - X_{-j}s)\beta_j - \frac{\tilde{a}}{2\tilde{b}}(X_j^TX_j+X_j^TX_{-j}t)\beta_j^2 - \frac{\lambda \Gamma(\tilde{a}+1/2)}{\Gamma(\tilde{a})\sqrt{\tilde{b}}}|\beta_j|,\\
	\end{aligned}
\end{equation}
where $s = \mu_{-j} - \Sigma_{-j,j}\Sigma_{j,j}^{-1}\mu_j$ and $t = \Sigma_{-j,j}\Sigma_{j,j}^{-1}$, $\tilde{a}$ and $\tilde{b}$ are posterior parameters for $\sigma^2$, $\mu,\Sigma$ are posterior parameters for $\beta$.
One of the key improvements of our method compared to MFVB is that it effectively captures the correlation between $\theta_2$ and $\theta_1$. Note that in \autoref{eq:MarLike}, the expectation with respect to $q(\theta_2|\theta_1)$ is performed. 
To illustrate this, let's consider the case of MFVB, where an expectation is taken with respect to $q(\theta)$ under the assumption of independence among the parameters $\theta$. Furthermore, it is evident that our method reduces to the MFVB approach when $t = 0$, resulting in $s = \mu_{-j}$. In the following subsection, we introduce the Lasso distribution.

\section{Lasso distribution}
% This part goes after subsection "3.3.1 Univariate Lasso Distribution" So, first describe the lasso distribution and its density and Z function then talk about these matches

\begin{figure}[h]
	\includegraphics[width=\linewidth]{Lasso_distribution}
	\caption{Visualization of the univariate Lasso distribution PDF for different parameter settings}
	\label{fig:LassoDist}
\end{figure}
\autoref{fig:LassoDist} demonstrates the shape, scale, and location of the univaraite Lasso distribution with different parameter settings which are shown in the legends.
From the yellow and dark-blue line, we can observe that parameter A control the size of the curvature of the tuning point, larger $a$ implies a smoother turning point.
From the red line, yellow line, and sky-blue line, we can observe that changing in $b$ changes the location of the curve.
From the yellow line and green line, $c$ controls the sharpness of the curve, a larger c implies distribution with smaller variance and a discontinuous derivative.


\subsection{Univariate Lasso Distribution}
If $x \sim $ Lasso(a,b,c), then the probability density function can be written as:
\begin{equation}
	p(x,a,b,c) = Z^{-1}\exp\left(-\frac{1}{2}ax^2+bx-c|x|\right),
\end{equation}
where $a \geq 0, b \in \mathbb{R}, c \geq 0$, there are also certain restrictions to certain parameter settings:
\begin{itemize}
	\item $a$ and $c$ cannot be 0 simultaneously.
	\item When $a = 0$, Lasso distribution will become an asymmetric Laplace distribution.
	\item When $c = 0$, Lasso distribution will become a normal distribution.
\end{itemize}
The probability density function of univariate Lasso distribution can be divided up into four components:
\begin{itemize}
	\item A normalization constant $Z$, to enable the integration of the probability density function to be 1.
	\item A quadratic term $ax^2$ to control the curvature of the curve.
	\item A linear term $bx$ to control the location of the curve.
	\item An absolute term $c|x|$ to control the sharpness of the discontinuous derivative.
\end{itemize} 

Certain properties of a lasso distribution such as normalizing constant $Z$, expectation $\mathbb{E}(x)$, second moment $\mathbb{E}(x^2)$ and variance $\mathbb{V}(x)$ are necessary for our algorithm which is computed in the following.

\subsubsection{Basic Property}
\subsubsection{Derivation of normalizing constant}
The normalizing constant Z can be written as a function of $a$, $b$, $c$.
$$
\begin{array}{rl}
	Z(a,b,c)
	&  = \int_{-\infty}^\infty \exp\left[ -\tfrac{1}{2}ax^2 + bx - c|x| \right] dx
	\\ [2ex]
	&  
	= \int_0^\infty    \exp\left[ -\tfrac{1}{2}ax^2 + (b - c)x \right] dx
	+ \int_{-\infty}^0 \exp\left[ -\tfrac{1}{2}ax^2 + (b + c)x \right] dx
	\\ [2ex]
	& 
	= \int_0^\infty \exp\left[ -\tfrac{1}{2}ax^2 + (b - c)x \right] dx
	+ \int_0^\infty \exp\left[ -\tfrac{1}{2}ay^2 - (b + c)y \right] dy
	\\ [2ex]
	& 
	= \int_0^\infty \exp\left[ - \frac{(x - \mu_1)^2}{2\sigma^2} + \frac{\mu_1^2}{2\sigma^2} \right] dx
	+ \int_0^\infty \exp\left[ - \frac{(x - \mu_2)^2}{2\sigma^2} + \frac{\mu_2^2}{2\sigma^2} \right] dy
	\\ [2ex]	& 
	= \sqrt{2\pi\sigma^2}
	\left[  \exp\left\{  \frac{\mu_1^2}{2\sigma^2} \right\} \int_0^\infty \phi(x;\mu_1,\sigma^2) dx
	+       \exp\left\{  \frac{\mu_2^2}{2\sigma^2} \right\} \int_0^\infty \phi(y;\mu_2,\sigma^2) dy
	\right] 
	\\ [2ex]
	& 
	= \sqrt{2\pi\sigma^2}
	\left[  \exp\left\{  \frac{\mu_1^2}{2\sigma^2} \right\} \left\{ 1 - \Phi(-\mu_1/\sigma) \right\} 
	+       \exp\left\{  \frac{\mu_2^2}{2\sigma^2} \right\} \left\{ 1 - \Phi(-\mu_2/\sigma) \right\} 
	\right] 
	\\ [2ex]
	& 
	= \sqrt{2\pi\sigma^2}
	\left[  \exp\left(  \frac{\mu_1^2}{2\sigma^2} \right) \Phi\left(\frac{\mu_1}{\sigma} \right) 
	+       \exp\left(  \frac{\mu_2^2}{2\sigma^2} \right) \Phi\left( \frac{\mu_2}{\sigma} \right)  
	\right] 
	
	
	\\ [2ex]
	& 
	= 
	\sigma \left[ \frac{\Phi(\mu_1/\sigma)}{\phi(\mu_1/\sigma)}
	+ \frac{\Phi(\mu_2/\sigma)}{\phi(\mu_2/\sigma)}  \right] 
\end{array} 
$$

\subsubsection{Derivation of Moments}
Note, the expectation is the first moment, and the variance of lasso distribution can be computed by the property $\mathbb{V}(x) = \mathbb{E}[x^2]- \mathbb{E}[x]^2$.
$$
\begin{array}{rl}
		E(x^r)
		&  = Z^{-1} \int_{-\infty}^\infty x^r \exp\left[ -\tfrac{1}{2}ax^2 + bx - c|x| \right] dx
		\\ [2ex]
		& 
		= Z^{-1}  \int_0^\infty   x^r \exp\left[ -\tfrac{1}{2}ax^2 + (b - c)x \right] dx
		+ \int_{-\infty}^0 x^r \exp\left[ -\tfrac{1}{2}ax^2 + (b + c)x \right] dx
		\\ [2ex]
		& 
		=  Z^{-1}  \int_0^\infty x^r \exp\left[ -\tfrac{1}{2}ax^2 + (b - c)x \right] dx
		+ (-1)^r\int_0^\infty y^r \exp\left[ -\tfrac{1}{2}ay^2 - (b + c)y \right] dy
		\\ [2ex]
		& 
		= Z^{-1}  \sqrt{2\pi\sigma^2}
		\exp\left(  \frac{\mu_1^2}{2\sigma^2} \right) \int_0^\infty x^r \phi(x;\mu_1,\sigma^2) dx
		\\ [2ex]
		&  \qquad + (-1)^r    \sqrt{2\pi\sigma^2}   \exp\left(  \frac{\mu_2^2}{2\sigma^2} \right) \int_0^\infty y^r \phi(y;\mu_2,\sigma^2) dy
		
		\\ [2ex]
		& 
		= \frac{\sigma}{Z} \left[  
		\frac{\Phi(\mu_1/\sigma)}{\phi(\mu_1/\sigma)} \frac{\int_0^\infty x^r \phi(x;\mu_1,\sigma^2) dx}{\Phi(\mu_1/\sigma)}
		+ (-1)^r  \frac{\Phi(\mu_2/\sigma)}{\phi(\mu_2/\sigma)}  \frac{\int_0^\infty y^r \phi(y;\mu_2,\sigma^2) dy}{\Phi(\mu_2/\sigma)}
		\right] 
		
		\\ [4ex]
		& 
		= \frac{\sigma}{Z} \left[  
		\frac{\Phi(\mu_1/\sigma)}{\phi(\mu_1/\sigma)} 
		\mathbb{E}( A^r )
		+ (-1)^r  \frac{\Phi(\mu_2/\sigma)}{\phi(\mu_2/\sigma)}  \mathbb{E}( B^r )
		\right].
\end{array} 
$$




\noindent where $A\sim TN_+(\mu_1,\sigma^2)$, $B\sim TN_+(\mu_2,\sigma^2)$ and $TN_+$ is denotes the positively truncated normal distribution; $\mu_1 = (b-c)/a$, $\mu_2 = -(c + b)/a$ and $\sigma^2 = 1/a$.
Note that
$$
\mathbb{E}(A) = \mu_1 + \frac{\sigma \phi(\mu_1/\sigma)}{\Phi(\mu_1/\sigma)} = \mu_1 + \sigma \zeta_1(\mu_1/\sigma)
$$

\noindent and
$$
\mathbb{V}(A) = \sigma^2  \left[ 1 + \zeta_2(\mu_1/\sigma) \right] 
$$

\noindent where $\zeta_k(x) = d^k \log \Phi(x)/dx^k$,
$\zeta_1(t) = \phi(t)/\Phi(t)$, $\zeta_2(t) = -t\,\zeta_1(t) - \zeta_1(t)^2$.
Here
$\zeta_1(x)$ is the inverse Mills ratio. The function $\zeta_1(x)$ represents the inverse Mills ratio, which requires careful consideration. 
Hence,
$$
\mathbb{E}(A^2) = \mathbb{V}(A) + \mathbb{E}(A)^2 = \sigma^2  \left[ 1 + \zeta_2(\mu_1/\sigma) \right] + \left[\mu_1 + \sigma \zeta_1(\mu_1/\sigma) \right]^2
$$
\autoref{eq:MarLike} can be matched to a univaraite Lasso distribution as a local approximation of Bayesian Lasso posterior. In addition, the joint likelihood of a pair of variables can also be matched by a bivariate Lasso distribution.\

\noindent In the following subsection we introduce bivariate Lasso distribution and its features.

\subsection{Bivariate Lasso Distribution}
\label{bilasso}
If $x \sim \mbox{Bilasso}(A,b,c)$ then it has density given by
\begin{equation}
	p(x) = Z^{-1}\exp\left(-\frac{1}{2}x^TAx+b^Tx-c||x||_1\right),
\end{equation}
 where $A \in S_d^+$: positive definite matrix with dimension $d$, $b \in \mathbb{R}^2$, $c \geq 0$.\\

Similarly, the probability density function of bivariate Lasso distribution can be divided up into four components:
\begin{itemize}
	\item A normalization constant $Z$, to enable the integration of the probability density function to be 1.
	\item A quadratic term $x^TAx$ to control the curvature of the curve.
	\item A linear term $bx$ to control the location of the curve.
	\item $c||x||_1$ as $l_1$ norm of $x$ to control the sharpness of the discontinuous derivative.
\end{itemize} 
Similar to the univariate Lasso distribution, closed form normalizing constant is useful for constructing a valid probability density function, while expectation and covariance matrix are essential for the bivariate local global algorithm to calculate the local approximated mean and covariance.\\
 
\subsubsection{Derivation of Normalizing Constant}
The normalizing constant can be calculated via integrate the unnormalized probability density function.
$$
\begin{array}{rl}
	Z(a,b,c)
	& = \int_{-\infty}^\infty \int_{-\infty}^\infty \exp\left[ -\frac{1}{2}x^TAx +
	 b^Tx - c1^T|x|_1 \right] dx
	\\ [2ex]
	& \qquad 
	= \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}x^TAx + (b^T - c1^T)x \right] dx \\[2ex]
	& \qquad
	+ \int_0^\infty\int_{-\infty}^0 \exp\left[ -\frac{1}{2}x^TAx + (b^T - c[1,-1]^T)x \right] dx\\
	& \qquad
	+ \int_{-\infty}^0\int_0^\infty \exp\left[ -\frac{1}{2}x^TAx + (b^T - c[-1,1]^T)x \right] dx\\
	& \qquad
	+ \int_{-\infty}^0\int_{-\infty}^0 \exp\left[ -\frac{1}{2}x^TAx + (b^T + c1^T)x \right]dx
	
	\\ [2ex]
	&
	= \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}x^TAx + (b^T - c1^T)x \right] dx\\
	& \qquad
	
	+ \int^\infty_0\int^{\infty}_0 \exp\left[ -\frac{1}{2}x^TA^*x + (b_1-c,-b_2-c)^Tx \right] dx\\
	& \qquad
	+ \int_0^\infty\int_0^\infty   \exp\left[ -\frac{1}{2}x^TA^*x + (-b_1-c,b_2-c)^Tx \right] dx\\
	& \qquad
	
	+ \int_0^\infty\int_0^\infty   \exp\left[ -\frac{1}{2}x^TAx - (b^T + c1^T)x \right]dx
	
	\\ [2ex]
	
	&
	= \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \frac{(A\mu_1)^T\Sigma_1(A\mu_1)]}{2} \right] dx\\
	& \qquad	
	+ \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) + \frac{(A^*\mu_2)^T\Sigma_2(A^*\mu_2)]}{2} \right] dx\\
	& \qquad
	+ \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}(x-\mu_3)^T\Sigma_2^{-1}(x-\mu_3) + \frac{(A^*\mu_3)^T\Sigma_2(A^*\mu_3)]}{2} \right] dx\\
	& \qquad	
	+ \int_0^\infty\int_0^\infty    \exp\left[ -\frac{1}{2}(x-\mu_4)^T\Sigma_1^{-1}(x-\mu_4) + \frac{(A\mu_4)^T\Sigma_1(A\mu_4)]}{2} \right] dx	
	
	\\ [2ex]
	&	 
	=  2\pi|\Sigma_1|^{\frac{1}{2}}[ \exp \left[ \frac{(A\mu_1)^T\Sigma_1(A\mu_1)}{2} \right] \int_0^\infty\int_0^\infty \phi_2(x;\mu_1,\Sigma_1)dx 	\\
	& \qquad
	+  \exp\left[ \frac{(A\mu_4)^T\Sigma_1(A\mu_4)}{2} \right] \int_0^\infty\int_0^\infty \phi_2(x;\mu_4,\Sigma_1)dx]\\
	& \qquad	
	+ 2\pi|\Sigma_2|^{\frac{1}{2}}[\exp\left[ \frac{(A^*\mu_2)^T\Sigma_2(A^*\mu_2)]}{2} \right] \int_0^\infty\int_0^\infty \phi_2(x;\mu_2,\Sigma_2)dx\\
	& \qquad
	+  \exp\left[ \frac{(A^*\mu_3)^T\Sigma_2(A^*\mu_3)}{2} \right] \int_0^\infty\int_0^\infty \phi_2(x;\mu_3,\Sigma_2)dx]\\
	
	
	&
	=|\Sigma_1| \left(\frac{\int_0^\infty\int_0^\infty \phi_2(x;\mu_1,\Sigma_1)dx}{\phi_2(A\mu_1,\Sigma_1^{-1})} 
	+ \frac{\int_0^\infty\int_0^\infty \phi_2(x;\mu_4,\Sigma_1)dx}{\phi_2(A\mu_4,\Sigma_1^{-1})}\right) \\
	& \qquad	
	+ |\Sigma_2| \left(\frac{\int_0^\infty\int_0^\infty \phi_2(x;\mu_2,\Sigma_2)dx}{\phi_2(A^*\mu_2,\Sigma_2^{-1})} + \frac{\int_0^\infty\int_0^\infty \phi_2(x;\mu_3,\Sigma_2)dx}{\phi_2(A^*\mu_3,\Sigma_2^{-1})} \right)
	
	
	
\end{array} 
$$

\noindent where $\mu_1 = A^{-1}(b-c1)^T$, $\mu_2 = A^{*-1}(b_1-c,-b_2-c)^T, \mu_3 = A^{*-1}(-b_1-c,b_2-c)^T, \mu_4 = A^{-1}(-b-c 1^T)^T $ and $\Sigma_1 = A^{-1}$, $\Sigma_2 = A^{*-1}$, $A^* = A \odot 	\begin{bmatrix}
	1 & -1\\
	-1 & 1\\
\end{bmatrix}$, here $\odot$ denotes the element-wise product.

\newpage

\subsubsection{Derivation of Expectation}
The expectation of lasso distribution can be derived by the expectation property: $\mathbb{E}[x] = \int xf(x)dx$. 
$$
\begin{array}{rl}
	\mathbb{E}[x] 
	& = Z^{-1} \int_{-\infty}^\infty \int_{-\infty}^\infty x \odot \exp\left[ -\frac{1}{2}x^TAx + b^Tx - c1^T||x||_1 \right] dx\\ [2ex]
	
	& \qquad
	= Z^{-1} \int_0^\infty\int_0^\infty x \odot   \exp\left[ -\frac{1}{2}x^TAx + (b^T - c1^T)x \right] dx\\
	
	& \qquad
	+ \int^\infty_0\int^{\infty}_0 [1,-1]^T \odot x \odot  \exp\left[ 
	-\frac{1}{2}x^TA^*\odot \begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix}x + (b_1-c,-b_2-c)^Tx \right] dx\\
	& \qquad
	+ \int_0^\infty\int_0^\infty  [-1,1]^T \odot x \odot    \exp\left[ 	
	-\frac{1}{2}x^TA^* \odot 	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix}x + (-b_1-c,b_2-c)^Tx \right] dx\\
	
	& \qquad
	
	- \int_0^\infty\int_0^\infty x \odot  \exp\left[ -\frac{1}{2}x^TAx + (b^T + c1^T)x \right]dx\\
	
	& \qquad
	=  Z^{-1}[|\Sigma_1|\left(\frac{\int_0^\infty\int_0^\infty x\odot\phi_2(x;\mu_1,\Sigma_1)dx}{\phi_2(A\mu_1,\Sigma_1^{-1})}
	-  \frac{\int_0^\infty\int_0^\infty x\odot\phi_2(x;\mu_4,\Sigma_1)dx}{\phi_2(A\mu_4,\Sigma_1^{-1}))}\right)\\
	& \qquad
	+ |\Sigma_2|
	\left([1,-1]^T\frac{\int_0^\infty\int_0^\infty x\odot\phi_2(x;\mu_2,\Sigma_2)dx}{\phi_2(A^*\mu_2,\Sigma_2^{-1})}
	+ [-1,1]^T   \frac{\int_0^\infty\int_0^\infty x\odot\phi_2(x;\mu_3,\Sigma_2)dx}{\phi_2(A^*\mu_3,\Sigma_2^{-1})})\right)]
	\\
	& \qquad
	=  Z^{-1}[|\Sigma_1|(\frac{\mathbb{E}[\textbf{A}]\int_0^\infty\int_0^\infty \phi_2(x;\mu_1,\Sigma_1)dx}{\phi_2(A\mu_1,\Sigma_1^{-1})}
	-  \frac{\mathbb{E}[D]\int_0^\infty\int_0^\infty \phi_2(x;\mu_4,\Sigma_1)dx}{\phi_2(A\mu_4,\Sigma_1^{-1})})\\
	& \qquad
	+ 
	|\Sigma_2|(
	[1,-1]^T \frac{\mathbb{E}[B]\int_0^\infty\int_0^\infty \phi_2(x;\mu_2,\Sigma_2)dx}{\phi_2(A^*\mu_2,\Sigma_2^{-1})}
	+ [-1,1]^T   \frac{\mathbb{E}[C]\int_0^\infty\int_0^\infty \phi_2(x;\mu_3,\Sigma_2)dx}{\phi_2(A^*\mu_3,\Sigma_2^{-1})})]
	\\
	&
	
	
\end{array}
$$
\noindent where $\mu_1 = A^{-1}(b-c1)^T$, $\mu_2 = A^{*-1}(b_1-c,-b_2-c)^T, \mu_3 = A^{*-1}(-b_1-c,b_2-c)^T, \mu_4 = A^{-1}(-b-c 1^T)^T $, $\Sigma_1 = A^{-1}$, $\Sigma_2 = A^{*-1}$, and $A^* = A \odot 	\begin{bmatrix}
	1 & -1\\
	-1 & 1\\
\end{bmatrix}$, here $\odot$ denotes the element-wise product.
\noindent $\textbf{A}\sim MTN_+(\mu_1,\Sigma_1)$, $B\sim MTN_+(\mu_2,\Sigma_2)$, $C\sim MTN_+(\mu_3,\Sigma_2)$, $D\sim MTN_+(\mu_4,\Sigma_1)$  denote the multivariate positively truncated normal distribution.
Note $\textbf{A}$ represents a multivariate positively truncated normal random variable, while $A$ represents the bivariate Lasso parameter.

\subsubsection{Derivation of Covariance Matrix}
The second moment of lasso distribution can be derived by the expectation property: $\mathbb{E}[xx^T] = \int xx^Tf(x)dx$.
$$
\mbox{Cov}(x) = \mathbb{E}[xx^T] - \mathbb{E}[x]\mathbb{E}[x]^T
$$
$$
\begin{array}{rl}
	\mathbb{E}[xx^T] 
	& = \int_{-\infty}^\infty \int_{-\infty}^\infty xx^T \odot \exp\left( -\frac{1}{2}x^TAx + b^Tx - c1^T||x||_1 \right) dx \\[2ex]
	& 
	= Z^{-1} 2\pi|\Sigma|^{\frac{1}{2}}[\exp\left( \frac{(A\mu_1)^T\Sigma(A\mu_1)]}{2} \right) \int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_1,\Sigma_1)dx\\
	& \qquad	
	+ 	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix} \odot \exp\left( \frac{(A^*\mu_2)^T\Sigma(A^*\mu_2)]}{2} \right) \int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_2,\Sigma_2)dx\\
	& \qquad
	+ 	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix}  \odot  \exp\left( \frac{(A^*\mu_3)^T\Sigma(A^*\mu_3)]}{2} \right) \int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_3,\Sigma_2)dx\\

	& \qquad	
	-  \exp\left( \frac{(A\mu_4)^T\Sigma(A\mu_4)]}{2} \right) \int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_4,\Sigma_1)dx]\\
	\\
	&
	=  Z^{-1}|\Sigma|[ \frac{\int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_1,\Sigma)dx}{\phi_2(A\mu_1,\Sigma)}
	
	+ 	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix} \odot  \frac{\int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_2,\Sigma)dx}{\phi_2(A\mu_2,\Sigma)}\\
	& \qquad
	+	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix} \odot   \frac{\int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_3,\Sigma_1)dx}{\phi_2(A\mu_3,\Sigma_1)}
	
	-  \frac{\int_0^\infty\int_0^\infty xx^T\odot\phi_2(x;\mu_4,\Sigma_1)dx}{\phi_2(A\mu_4,\Sigma_1)}]\\
	\\
	&
	=  Z^{-1}[|\Sigma_1|\left(\frac{\mathbb{E}[\textbf{A}\textbf{A}^T]\int_0^\infty\int_0^\infty \phi_2(x;\mu_1,\Sigma_1)dx}{\phi_2(A\mu_1,\Sigma_1^{-1})}
	+  \frac{\mathbb{E}[DD^T]\int_0^\infty\int_0^\infty \phi_2(x;\mu_4,\Sigma_1)dx}{\phi_2(A\mu_4,\Sigma_1^{-1})}\right)\\
	& \qquad
	+ |\Sigma_2|\left(
	\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix} \odot \frac{\mathbb{E}[BB^T]\int_0^\infty\int_0^\infty \phi_2(x;\mu_2,\Sigma_2)dx}{\phi_2(A^*\mu_2,\Sigma_2^{-1})}
	+\begin{bmatrix}
		1 & -1\\
		-1 & 1\\
	\end{bmatrix} \odot   \frac{\mathbb{E}[CC^T]\int_0^\infty\int_0^\infty \phi_2(x;\mu_3,\Sigma_2)dx}{\phi_2(A^*\mu_3,\Sigma_2^{-1})}\right)]
	\\
	
	
\end{array}
$$
\noindent where $\mu_1 = A^{-1}(b-c1)^T$, $\mu_2 = A^{*-1}(b_1-c,-b_2-c)^T, \mu_3 = A^{*-1}(-b_1-c,b_2-c)^T, \mu_4 = A^{-1}(-b-c 1^T)^T $ $\Sigma_1 = A^{-1}$, $\Sigma_2 = A^{*-1}$, and $A^* = A \odot 	\begin{bmatrix}
	1 & -1\\
	-1 & 1\\
\end{bmatrix}$, here $\odot$ denotes the element-wise product.
\noindent $\textbf{A}\sim MTN_+(\mu_1,\Sigma_1)$, $B\sim MTN_+(\mu_2,\Sigma_2)$, $C\sim MTN_+(\mu_3,\Sigma_2)$, and $D\sim MTN_+(\mu_4,\Sigma_1)$ denote the multivariate positively truncated normal distributions.
In addition, the second moment $\mathbb{E}[AA^T]$ and $\mathbb{E}[BB^T]$ can be derived similarly from the variance property in multivariate function:
$$
\mathbb{E}[AA^T] = \mbox{Cov}(A) - \mathbb{E}[A]\mathbb{E}[A]^T 
$$

\subsubsection{Derivation of marginal distribution}
The marginal distribution of the bivariate Lasso distribution is useful for evaluating the accuracy of $l_1$ norm approximation, allowing for comparison and visualization of the approximated density. The marginal distributions of $x_1$ and $x_2$ can be derived by $f(x_1)
=  \int_{-\infty}^{\infty} f(x_1,x_2)dx_2 $ and $	f(x_2) =  \int_{-\infty}^{\infty} f(x_1,x_2)dx_1 $, respectively. In the bivariate Lasso case, we have
$$
\begin{array}{rl}
	f(x_1)
	& =  \int_{-\infty}^{\infty} f(x_1,x_2)dx_2 \\
	& \qquad
	= Z^{-1}\int_{-\infty}^{\infty}\exp(-\frac{1}{2}x^TAx + b^Tx - c||x||_1))dx_2 \\
	& \qquad
	= Z^{-1}\int_{-\infty}^{\infty}\exp(-0.5a_{11}x_1^2  + b_1x_1 - c|x_1|)dx_2 \\
	& \qquad
	\int_{-\infty}^{\infty}\exp(-\frac{1}{2} [(a_{12}+a_{21})x_1x_2 + a_{22}x_2^2] + b_2x_2 - c|x_2|]dx_2\\
	& \qquad
	= k \int_{-\infty}^{\infty} \exp[-(0.5(a_{12}+a_{21})x_1x_2 -0.5a_{22}x_2^2 + b_2x_2 - c|x_2|]dx_2\\
	& \qquad
	= k[ \int_{0}^{\infty} \exp[-0.5(a_{12}+a_{21})x_1x_2 - 0.5a_{22}x_2^2 + (b_2 - c)x_2 ]dx_2  \\
	& \qquad
	+\int_{-\infty}^{0} \exp[-0.5(a_{12}+a_{21})x_1x_2  - 0.5a_{22}x_2^2  + (b_2+c)x_2]dx_2 \\
	& \qquad
	= k[ \int_{0}^{\infty} \exp[-0.5(a_{12}+a_{21})x_1x_2 - 0.5a_{22}x_2^2 + (b_2 - c)x_2 ]dx_2  \\
	& \qquad
	+\int_{0}^{\infty} \exp[0.5(a_{12}+a_{21})x_1x_2  - 0.5a_{22}x_2^2  - (b_2+ c)x_2]dx_2 ]\\
	& \qquad
	= k[\int_{0}^{\infty} \exp[-\frac{(x_2-\mu_1)^2}{2\sigma^2} + \frac{\mu_1^2}{2\sigma^2}]dx_2] + \int_{0}^{\infty} \exp[-\frac{(x_2-\mu_2)^2}{2\sigma^2} + \frac{\mu_2^2}{2\sigma^2}]dx_2] \\
	& \qquad
	= k \sigma\left[\frac{\Phi(\mu_1/\sigma)}{\phi(\mu_1/\sigma)} +
	\frac{\Phi(\mu_2/\sigma)}{\phi(\mu_2/\sigma)}\right]  \\
	
\end{array}
$$
where $\mu_1 = (-\frac{a_{12}+a_{21}}{2a_{22}}x_1 + \frac{b_2-c}{a_{22}}) $, $\mu_2 =(\frac{a_{12}+a_{21}}{2a_{22}}x_1 - \frac{b_2+c}{a_{22}}) $, $\sigma^2 = 1/a_{22}$, $k =  Z^{-1}\exp(-0.5a_{11}x_1^2 + b_1x_1 - c|x_1|)$. Also, the marginal distribution of $x_2$ is
$$
\begin{array}{rl}
	f(x_2)
	& =  \int_{-\infty}^{\infty} f(x_1,x_2)dx_1 \\
	& \qquad
	= Z^{-1}\int_{-\infty}^{\infty}\exp(-\frac{1}{2}x^TAx + b^Tx - c||x||_1))dx_1 \\
	& \qquad
	= Z^{-1}\int_{-\infty}^{\infty}\exp(-0.5 a_{22}x_2^2 + b_2x_2 - c|x_2|) dx_1\\
	& \qquad
	\int_{-\infty}^{\infty}\exp(-\frac{1}{2} [a_{12}a_{21}x_1x_2 + a_{11}x_1^2] + b_1x_1 - c|x_1|]dx_1\\
	& \qquad
	= k \int_{-\infty}^{\infty} \exp[-0.5(a_{12}+a_{21})x_1x_2 -0.5a_{11}x_1^2 + b_1x_1 - c|x_1|]dx_1\\
	& \qquad
	= k[ \int_{0}^{\infty} \exp[-0.5(a_{12}+a_{21})x_1x_2 - 0.5a_{11}x_1^2 + (b_1 - c)x_1 ]dx_1  \\
	& \qquad
	+\int_{-\infty}^{0}\exp[-0.5(a_{12}+a_{21})x_1x_2  - 0.5a_{11}x_1^2  + (b_1 + c)x_1]dx_1 ]\\
	& \qquad
	= k[ \int_{0}^{\infty} \exp[-0.5(a_{12}+a_{21})x_1x_2 - 0.5a_{11}x_1^2 + (b_1 - c)x_1 ]dx_1  \\
	& \qquad
	+\int_{0}^{\infty} \exp[0.5(a_{12}+a_{21})x_1x_2  - 0.5a_{11}x_1^2  - (b_1+c)x_1]dx_1 ]\\
	& \qquad
	= k[\int_{0}^{\infty} \exp[-\frac{(x_2-\mu_1)^2}{2\sigma^2} + \frac{\mu_1^2}{2\sigma^2}]dx_1] + \int_{0}^{\infty} \exp[-\frac{(x_2-\mu_2)^2}{2\sigma^2} + \frac{\mu_2^2}{2\sigma^2}]dx_1] \\
	& \qquad
	= k \sigma\left[\frac{\Phi(\mu_1/\sigma)}{\phi(\mu_1/\sigma)} +
	\frac{\Phi(\mu_2/\sigma)}{\phi(\mu_2/\sigma)}\right]  \\
	
\end{array}
$$
where $\mu_1 = \left(-\frac{a_{12}+a_{21}}{2a_{11}}x_2 + \frac{b_1-c}{a_{11}}\right) $, $\mu_2 =\left(\frac{a_{12}+a_{21}}{2a_{11}}x_2 - \frac{b_1+c}{a_{11}}\right) $, $\sigma^2 = 1/a_{11}$,\\ $k =  Z^{-1}\exp\left(-0.5a_{22}x_2^2 + b_2x_2 - c|x_2|\right)$.


\section{Local-Global Algorithm}
\subsection{Univariate local global algorithm}
The global approximation is a marginal normal approximation to the conditional distribution $p(\beta_j|\mathcal{D})$: $q^*(\beta_j) \approx N(\mu_j^*,\Sigma_{jj}^*)$. The local approximation of mean $\mu_j^*$ and variance $\Sigma_{jj}^*$ can be obtained by the expression of Lasso expectation formula with given Lasso parameters $a$, $b$, and $c$. Based on \autoref{eq:MarLike}, we can show that the distribution of $\beta_j$ given $\mathcal{D}$ is Lasso distribution, $p(\beta_j|\mathcal{D}) \propto p(\beta_j,\mathcal{D}) \sim Lasso\left(\frac{\tilde{a}}{\tilde{b}}(y - x_{-j}s), \frac{\tilde{a}}{2\tilde{b}}(X_j^TX_j+X_j^TX_{-j}t) , \frac{\lambda \Gamma(\tilde{a}+1/2)}{\Gamma(\tilde{a})\sqrt{\tilde{b}  }}\right) $, where $s = \mu_{-j} - \Sigma_{-j,j}\Sigma_{j,j}^{-1}\mu_j$, $t = \Sgima_{-j,j}\Sigma_{j,j}^{-1}$ are predefined value from original data, $\tilde{a}$ and $\tilde{b}$ are from MFVB output. 
Matching moments of $p(\beta_j|\mathcal{D})$ with a normal distribution and reforming the joint distribution we obtain
\autoref{eq:updateQ}:
\begin{equation}
	\label{eq:updateQ}
	q^*(\beta) = q(\beta_{-j}|\beta_j)\phi(\beta_j;\mu_j^*,\Sigma_{jj}^*).
\end{equation} 
Since both $q(\beta_{-j}|\beta_j)$ and $q(\beta_j)= \phi(\beta_j;\mu_j^*,\Sigma_{jj}^*)$ are normal distributions, the joint distribution is also normal distribution. Suppose the mean and covariance of the joint distribution
are $\widetilde{\mu}$ and $\widetilde{\Sigma}$ respectively, then $\widetilde{\mu}_1 = \mu_1^*$, and $\widetilde{\Sigma}_{11} = \Sigma_{11}^*$. The derivation of $\tilde{\mu}$ is shown below based on the law of total expectation.
Denote $\theta_1 = \beta_j$, $\theta_2 = \beta_{-j}$ as in Chapter 1. It is known that $q(\theta_2|\theta_1)$ and $q(\theta_1)$ are Gaussian since $q(\theta)$ is Gaussian. Hence, their joint distribution is also Gaussian. The 
mean and variance of the joint distribution of $\theta_1$ are $\mu_1^*$
and $\Sigma_{11}^*$. 
The rest of the derivation is to determine $\widetilde{\mu}_{2}$, $\widetilde{\Sigma}_{22}$
and $\widetilde{\Sigma}_{12}$. Denote the actual $q(\theta) \sim N(\mu,\Sigma)$, which approximate $p(\theta|\mathcal{D})$

\bigskip 
\noindent We have
$$
\begin{array}{rl}
	& \mathbb{E}[\theta_2] = \mathbb{E}[\mathbb{E}(\theta_2\mid\theta_1)] 
	\\ [1ex]
	&  = \mathbb{E}[\mu_2 + \Sigma_{21}\Sigma_{11}^{-1}\left(\theta_1 - \mu_1\right)]
	\\ [1ex]
	& = \mu_2 + \Sigma_{21}\Sigma_{11}^{-1}\left(\mu_1^* - \mu_1\right).
\end{array} 
$$

\noindent Similarly,
$$
\begin{array}{rl}
	\mbox{Cov}(\theta_2) 
	&  = \mathbb{E}[\mbox{Cov}(\theta_2\mid\theta_1)] + \mbox{Cov}[\mathbb{E}(\theta_2\mid\theta_1)] 
	\\ [2ex]
	&  = \mathbb{E}(\Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1}\Sigma_{12}) + \mbox{Cov}[\mu_2 + \Sigma_{21}\Sigma_{11}^{-1}\left(\theta_1 - \mu_1\right)] 
	\\ [2ex]
	&  = \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1}\Sigma_{12}
	+ \Sigma_{21}\Sigma_{11}^{-1}  \mbox{Cov}(\theta_1) \Sigma_{11}^{-1}  \Sigma_{12}
	\\ [2ex]
	&  =  \Sigma_{22} 
	+ \Sigma_{21}(\Sigma_{11}^{-1}  \Sigma_{11}^* \Sigma_{11}^{-1} -\Sigma_{11}^{-1})  \Sigma_{12}.
\end{array} 
$$

\noindent Lastly,
$$
\begin{array}{rl}
	\mbox{Cov}(\theta_1,\theta_2) 
	&  = \mathbb{E}[(\theta_1 - \mathbb{E}(\theta_1))(\theta_2 - \mathbb{E}(\theta_2))^T] 
	\\ [2ex]
	&  = \left[ \mathbb{E}\left\{ (\theta_1 - \mathbb{E}(\theta_1))(\theta_2 - \mathbb{E}(\theta_2))^T \mid \theta_1 \right\} \right]     \\ [2ex]
	& = \mathbb{E}\left[ \left(\theta_1 - \mu_1^* \right)\left(  \Sigma_{21}\Sigma_{11}^{-1}(\theta_1 - \mu_1^*)    \right)^T\right] 
	\\ [2ex]
	&  = \mathbb{E}\left[ \left(\theta_1 - \mu_1^* \right)\left(  \theta_1 - \mu_1^* \right)^T\right] \Sigma_{11}^{-1}\Sigma_{12}
	\\ [2ex]
	&  = \Sigma_{11}^* \Sigma_{11}^{-1}\Sigma_{12}.
\end{array} 
$$

\noindent Hence, $q^*(\beta) = N(\widetilde{\mu},\widetilde{\Sigma})$ where
$$
\widetilde{\mu} =
\left[ \begin{array}{c}
	\mu_1^* \\
	\mu_2 + \Sigma_{21}\Sigma_{11}^{-1}\left(\mu_1^* - \mu_1\right)
\end{array} \right].
$$
Based on the law of total variance $\tilde{\Sigma}$  is:
$$
\widetilde{\Sigma} = 
\left[ \begin{array}{cc}
	\Sigma_{11}^* & \Sigma_{11}^* \Sigma_{11}^{-1}\Sigma_{12} \\
	\Sigma_{21}  \Sigma_{11}^{-1}\Sigma_{11}^* & \Sigma_{22} 
	+ \Sigma_{21}\Sigma_{11}^{-1}  ( \Sigma_{11}^* -\Sigma_{11})  \Sigma_{11}^{-1} \Sigma_{12}
\end{array} \right].
$$




\noindent Hence, $q^*(\beta) = N(\widetilde{\mu},\widetilde{\Sigma})$ at the time when updating variable $\beta_j$ is:
\begin{equation}
	\label{eq:LG_mu}
	\widetilde{\mu} =
	\left[ \begin{array}{c}
		\mu_j^* \\
		\mu_{-j} + \Sigma_{-j,j}\Sigma_{jj}^{-1}\left(\mu_j^* - \mu_j\right)
	\end{array} \right]
\end{equation}

\noindent and
\begin{equation}
	\label{eq:LG_sigma}
	\widetilde{\Sigma} = 
	\left[ \begin{array}{cc}
		\Sigma_{jj}^* & \Sigma_{jj}^* \Sigma_{jj}^{-1}\Sigma_{j-j} \\
		\Sigma_{-jj}  \Sigma_{jj}^{-1}\Sigma_{jj}^* & \Sigma_{-j-j} 
		+ \Sigma_{-jj}\Sigma_{jj}^{-1}  ( \Sigma_{jj}^* -\Sigma_{jj})  \Sigma_{jj}^{-1} \Sigma_{j-j}
	\end{array} \right].
\end{equation}
In practise, since we do not know the actual $\mu$ and $\Sigma$, they will be replaced by initial $\widetilde{\mu}$ and initial $\widetilde{\Sigma}$ respectively, as denoted in the below algorithm.
\newpage
\begin{algorithm}
	\caption{Univariate Local-Global Algorithm}
	\begin{algorithmic}[1]
		
    \State Input: $X$, $y$, parameters $(\tilde{a},\tilde{b},\tilde{\mu},\tilde{\Sigma})$ from MFVB , $\lambda$
	\While{$\tilde{\mu}$ is changing less than $\epsilon$}
	\For{$j=1$ to $p$}
	\State $a = \frac{\tilde{a}}{\tilde{b}}(X^TX)_{j,j} + (X^TX)_{j,-j}\tilde{\Sigma}_{-j,j}\tilde{\Sigma}_{j,j}^{-1}$ \Comment{Obtain Lasso parameter}
	\State $b = \frac{\tilde{a}}{\tilde{b}}  X_{j}(y-X_{-j}(\tilde{\mu}_{-j} - \tilde{\Sigma}_{-j,j}\tilde{\Sigma}_{j,j}^{-1}\tilde{\mu}_j))       $ \Comment{Obtain Lasso parameter}
	\State $c = \lambda (\exp(\Gamma(\tilde{a}+0.5) - \Gamma(\tilde{a}) - 0.5\log(\tilde{b})))  $ \Comment{Obtain Lasso parameter}
	\State $\tilde{\mu_{j}} =  elasso(a,b,c) = \mu_j ^*$ \Comment{Replace by Local mean}
	\State $\tilde{\mu}_{-j} = \tilde{\mu}_{-j} +  \tilde{\Sigma}_{-j,j}\tilde{\Sigma}^{-1}_{jj}(\tilde{\mu}_j^*-\tilde{\mu}_j)$ \Comment{Update Global Mean}
	\State $\tilde{\Sigma}_{j,j} = vlasso(a,b,c) = \Sigma_{jj}^*$ \Comment{Replace by Updated Local Covariance}
	\State $\tilde{\Sigma}_{j,-j} = \Sigma_{jj}^*  \tilde{\Sigma}_{jj}^{-1}\tilde{\Sigma}_{j,-j} $ \Comment{Update Global Covariance}
	\State $\tilde{\Sigma}_{-j,j} = \tilde{\Sigma}_{j,-j}^T$ \Comment{Update Global Covariance}
	\State $\tilde{\Sigma}_{-j,-j} = \tilde{\Sigma}_{-j,-j} + \tilde{\Sigma}_{-j,j}\tilde{\Sigma}_{j,j}^{-1}(\Sigma_{j,j}^{*} - \tilde{\Sigma}_{j,j})\tilde{\Sigma}_{j,j}^{-1}\tilde{\Sigma}_{j,-j}$ \Comment{Update Global Covariance}		
	\EndFor
	\EndWhile
	\State return $\tilde{\mu},\tilde{\Sigma}$
	\end{algorithmic}
\end{algorithm}
\noindent The following bullet points summarize the procedure of our method.
\begin{itemize}
	\item Our target: $p(\theta|\mathcal{D})$ = $p(\beta, \sigma^2|\mathcal{D})$, where a normal distribution is used to approximate with global parameters $\tilde{\mu}$ and $\tilde{\Sigma}$. The initial estimate of $\tilde{\mu}$ and $\tilde{\Sigma}$ is obtained via MFVB. Define $\beta = (\beta_j,\beta_{-j})$, separated by a target variable and the rest of the variables. $\mu_{j}^*$ and $\Sigma_{jj}^*$ are local parameters.
	
	
	\item Compute the mean and variance of the Lasso distribution: $\text{elasso}$ and $\text{vlasso}$ functions in the aforementioned algorithm, with the current values $a$, $b$, and$c$: $q^*(\theta_1) \approx N(\mu_j^*,\Sigma_{jj}^*)$.
	\item Use the update expression $q^*(\beta) = q(\beta_{-j}|\beta_j)\phi(\beta_j;\mu_j^*,\Sigma_{jj}^*)$ to adjust the global mean and global covariance of $q(\beta)$.
	\item Iterate through each variable $\beta_j$ and update $\tilde{\mu}$ and $\tilde{\Sigma}$ each time by the derived expression.
\end{itemize}

\newpage
\subsection{Bivariate local global algorithm}
Apart from the univariate algorithm that matches each variable of interest into a univariate Lasso distribution, each pair of regression coefficients can be matched simultaneously by a bivariate Lasso distribution as mentioned in \autoref{bilasso}.
The number of index pairs can be calculated while the combination arithmetic. The total number of index pairs is ${p}\choose{2}$. For example, if $p=4$ then index pair $\mathcal{I}$: can be $(1,2)$, $(1,3)$, $(1,4)$, $(2,3),(2,4), (3,4)$.
The main intuition is similar to \autoref{LocalGlobalVI}, but
the log likelihood of each index pair $\mathcal{I}$ now can be written as:
\begin{equation}
	\begin{array}{rl} 
		\log p(D,\beta_\sI) 
		& \ds = \mathbb{E}_{\beta_{-\sI},\sigma^2\mid \mathcal{D},\beta_\sI} \log p(\beta_\sI\mid \mathcal{D},\beta_{-\sI},\sigma^2)
		\\ [2ex]
		& \ds \approx  \mathbb{E}_{q(\beta_{-\sI}\mid\beta_\sI)q(\sigma^2)} \log p(\beta_\sI\mid \mathcal{D},\beta_{-\sI},\sigma^2).
		% & \ds =  \frac{\widetilde{a}}{\widetilde{b}} \mX_j^T\left( \vy - \mX_{-j}\vs \right)  \beta_j
		% - \frac{\widetilde{a}}{2\widetilde{b}} \left( \mX_j^T\mX_j + \mX_j^T\mX_{-j}\vt   \right)\beta_j^2
		% - \frac{\lambda\,\Gamma(\widetilde{a}+1/2)}{\Gamma(\widetilde{a})\sqrt{\widetilde{b}}} |\beta_j| 
		
	\end{array}
\end{equation}
The log-likelihood of index pair can be written as the following:
\begin{equation}
	\begin{array}{rl}
		\log p(\beta_\sI\mid\mathcal{D},\beta_{-I},\sigma^2) 
		& \ds = - \tfrac{1}{2\sigma^2}\beta_\sI^TX_\sI^TX_\sI\beta_\sI - \frac{\lambda}{\sigma} \|\beta_{\sI} \|_1
		\\ [2ex]
		& \ds \ \ \  
		+ \frac{1}{2\sigma^2}(y - X_{-\sI}\beta_{-\sI})^TX_\sI\beta_{\sI}
		+ \frac{1}{2\sigma^2}\beta_{\sI}X_\sI^T(y - X_{-\sI}\beta_{-\sI}).
		
	\end{array}
\end{equation}
For the purpose of enabling $\beta_{-\sI}$ to be symmetric for preventing generating a non-symmetric $A$, the log-likelihood of index pair $\sI$ can be remodified as the following, denoting \noindent 
$S =  \mu_{-\sI} - T \mu_\sI \in \mathbb{R}^{|\sI|},
T = \Sigma_{-\sI,\sI} \Sigma_{\sI,\sI}^{-1} \in \mathbb{R}^{(p - |\sI|)\times|\sI|}
$ similar to the univariate case.
\begin{equation}
	\begin{array}{rl} 
		\log p(\mathcal{D},\beta_\sI) 
		&   =  \frac{\widetilde{a}}{\widetilde{b}} \left( y - X_{-\sI}S \right)^T X_\sI \beta_\sI
		- \frac{\lambda\,\Gamma(\widetilde{a}+1/2)}{\Gamma(\widetilde{a})\sqrt{\widetilde{b}}} |\beta_\sI\|_1
		\\ [2ex]
		& 
		- \frac{\widetilde{a}}{2\widetilde{b}} \beta_\sI^T \left( X_\sI^TX_\sI + \tfrac{1}{2}X_\sI^TX_{-\sI}T 
		+ \tfrac{1}{2}T X_{-\sI}^TX_\sI
		\right)\beta_\sI.
	\end{array}
\end{equation}
Then the marginal log-likelihood can be matched to a bivariate Lasso distribution:\\
\begin{equation}
	\beta_{\sI}\mid\mathcal{D} 
	\stackrel{\mbox{\scriptsize approx.}}{\sim} 
	\mbox{BiLasso}\left(
	\frac{\widetilde{a}}{\widetilde{b}} \left(X_\sI^TX_\sI + \tfrac{1}{2}X_\sI^TX_{-j}T + \tfrac{1}{2}T^T X_{-j}^T X_\sI \right),
	\frac{\widetilde{a}}{\widetilde{b}} X_\sI^T\left( y - X_{-\sI}S \right),
	\frac{\lambda\,\Gamma(\widetilde{a}+1/2)}{\Gamma(\widetilde{a})\sqrt{\widetilde{b}}} 
	\right).
\end{equation}
In order to update and refine the global mean and variance parameters, it is important to consider the conditional distribution of $q(\beta_{-\sI}|\beta_\sI)$. This distribution can be expressed in the following manner:
\begin{equation}
	\begin{array}{rl}
		q(\beta_{-\sI}\mid\beta_\sI) 
		& = N_{p-|\sI|}\left( \mu_{-\sI} +\Sigma_{-\sI,\sI} \Sigma_{\sI,\sI}^{-1}(\beta_\sI - \mu_\sI), \Sigma_{-\sI,-\sI} - \Sigma_{-\sI,\sI}\Sigma_{\sI,\sI}^{-1}\Sigma_{\sI,-\sI}  \right)
		\\ 
		& = N_{p-|\sI|}\left( S  + T \beta_{\sI}, \Sigma_{-\sI,-\sI} - \Sigma_{-\sI,\sI}\Sigma_{\sI,\sI}^{-1}\Sigma_{\sI,-\sI}  \right).
	\end{array}
	\label{eq:condBiNormal}
\end{equation}

By utilizing \autoref{eq:condBiNormal} and extending the propagation equation derived from \autoref{eq:updateQ}, the update formula for $\tilde{\mu}$ and $\tilde{\Sigma}$ can be modified using the same equation as in the univariate case, as presented in \autoref{eq:LG_mu} and \autoref{eq:LG_sigma}. The only alteration required is the substitution of the subscript $j$ with the index pair $\mathcal{I}$.

\begin{algorithm}
	\caption{Bivariate Local-Global Algorithm}
	\begin{algorithmic}[1]
		
		\State Input: $X$, $y$, parameters $(\tilde{a},\tilde{b},\tilde{\mu},\tilde{\Sigma})$ from MFVB, $\lambda$
		\While{$\tilde{\mu}$ is changing less than $\epsilon$}
		\For{$ \mathcal{I} = (1,1)$ to $(p-1, p)$}
		\State $a = \frac{\widetilde{a}}{\widetilde{b}} \left(X_\sI^TX_\sI + \tfrac{1}{2}X_\sI^TX_{-j}T + \tfrac{1}{2}T^T X_{-j}^T X_\sI \right)$ \Comment{Obtain Lasso parameter}
		\State $b = \frac{\widetilde{a}}{\widetilde{b}} X_\sI^T\left( y - X_{-\sI}s \right) $ \Comment{Obtain Lasso parameter}
		\State $c = \lambda (\exp(\Gamma(\tilde{a}+0.5) - \Gamma(\tilde{a}) - 0.5\log(\tilde{b})))  $ \Comment{Obtain Lasso parameter}
		\State $\tilde{\mu}_{\mathcal{I}} =  Bielasso(a,b,c) = \mu_\mathcal{I} ^*$ \Comment{Replace by Local mean}
		\State $\tilde{\mu}_{-\mathcal{I}} = \tilde{\mu}_{-\mathcal{I}} +  \tilde{\Sigma}_{-\mathcal{I},\mathcal{I}}\tilde{\Sigma}^{-1}_{\mathcal{I}\mathcal{I}}(\tilde{\mu}_\mathcal{I}^*-\tilde{\mu}_\mathcal{I})$ \Comment{Update Global Mean}
		\State $\tilde{\Sigma}_{\mathcal{I},\mathcal{I}} = Bivlasso(a,b,c) = \Sigma_{\mathcal{I}\mathcal{I}}^*$ \Comment{Replace by Updated Local Covariance}
		\State $\tilde{\Sigma}_{\mathcal{I},-\mathcal{I}} = \Sigma_{\mathcal{I}\mathcal{I}}^*  \tilde{\Sigma}_{\mathcal{I}\mathcal{I}}^{-1}\tilde{\Sigma}_{\mathcal{I},-\mathcal{I}} $ \Comment{Update Global Covariance}
		\State $\tilde{\Sigma}_{-\mathcal{I},\mathcal{I}} = \tilde{\Sigma}_{\mathcal{I},-\mathcal{I}}^T$ \Comment{Update Global Covariance}
		\State $\tilde{\Sigma}_{-\mathcal{I},-\mathcal{I}} = \tilde{\Sigma}_{-\mathcal{I},-\mathcal{I}} + \tilde{\Sigma}_{-\mathcal{I},\mathcal{I}}\tilde{\Sigma}_{\mathcal{I},\mathcal{I}}^{-1}(\Sigma_{\mathcal{I},\mathcal{I}}^{*} - \tilde{\Sigma}_{\mathcal{I},\mathcal{I}})\tilde{\Sigma}_{\mathcal{I},\mathcal{I}}^{-1}\tilde{\Sigma}_{\mathcal{I},-\mathcal{I}}$ \Comment{Update Global Covariance}		
		\EndFor
		\EndWhile
		\State return $\tilde{\mu},\tilde{\Sigma}$
	\end{algorithmic}
\end{algorithm}
\noindent The following bullet points summarize the procedure of Bivariate Local-Global Algorithm
\begin{itemize}
	\item Our target: $p(\theta|\mathcal{D})$ = $p(\beta, \sigma^2|\mathcal{D})$, where a normal distribution is used to approximate with global parameters $\tilde{\mu}$, and $\tilde{\Sigma}$. The initial estimate of $\tilde{\mu}$ and $\tilde{\Sigma}$ has been obtained via MFVB. Define $\theta = (\beta_j,\beta_{-j})$, separated by a target variable and the rest of the variables. $\mu_{j}^*$, and $\Sigma_{jj}^*$ as local parameter.
	\item Compute the mean and variance of bivariate Lasso distribution: $\text{bielasso}$ and $\text{bivlasso}$ functions in the aforementioned algorithm, with the current $a$, $b$, and $c$: $q^*(\beta_j) \approx N(\mu_j^*,\Sigma_{jj}^*)$.
	\item Use the update expression $q^*(\theta) = q(\beta_{-j}|\beta_j)\phi(\beta_j;\mu_j^*,\Sigma_{jj}^*)$ to adjust the global mean and global covariance of $q(\beta)$.
	\item Iterates through each variable $\beta_j$ and update $\tilde{\mu}$ and $\tilde{\Sigma}$ each time by the derived expression.
\end{itemize}

Nevertheless, the Bivariate-Local-Global algorithm has an issue currently. One of the main issues can be the result of generating nan value for $\tilde{\mu}$ and components in $\tilde{\Sigma}$. The origin of this problem has not been found until now. Due to the limited amount of time, we would like to resolve this issue in the future. As a consequence, we would not experiment with the effectiveness and the approximation of it in the next Chapter.