\chapter{Discussion and Conclusion}

\section{Discussion}
The experiment results presented in the previous chapter provide insights into the performance of different algorithms in terms of approximation accuracy and density plots. The following six bullet points highlight notable observations from the experiments:
\begin{itemize}
    \item MFVB tends to generate densities with less variance, resulting in higher concentration at the center.
    \item The marginal posterior distribution obtained from the Local-Global algorithm demonstrates high accuracy and versatility in accommodating various predictor distributions beyond the normal distribution.
    \item The global posterior distribution obtained from the Local-Global algorithm exhibits greater accuracy compared to MFVB.
    \item The running time of MFVB is slower than that of the Local-Global algorithms, both for local and global approximation.
    \item The Local-Global algorithm maintains high accuracy even when there is a high correlation between predictors, as observed in the results from the Hitters dataset.
    \item The Local-Global algorithm achieves high accuracy even when the number of predictors is higher than the sample size, as evidenced by the results from the Eyedata dataset.
\end{itemize}
These bullet points effectively summarize the notable phenomena observed in the study. Further explanations reveal that the variance of the density produced by MFVB tends to be less because of the adaptability of the function form of the univariate Lasso distribution. The addition of local adjustment by capturing the correlation between $\beta_j$ and $\beta_{-j}$ makes the global approximation from the Local-Global algorithm more accurate than MFVB. Although the Local-Global algorithm is slightly slower than MFVB due to an extra procedure for the local approximation process and calculation of moments for Lasso distribution, both algorithms are faster than MCMC. Additionally, the Local-Global algorithm's high accuracy in the presence of a high correlation between predictors and more predictors than samples demonstrates the algorithm's adaptability and adjustment, leading to better approximation results.











%Bodyfat, Prostate, Credit: no correlation; VB does do well because of that
%
%Kakadu n = 2000;
%
%Hard for VB: Hitters: high correlation(between predictors) and Eyedata: p > n.


\label{Chapter5}
\section{Limitations}
The two bullet points effectively summarize the limitations:

\begin{itemize}
    \item The automatic choice of $\lambda$ is still obtained by Gibbs Sampling.
    \item The Univariate Local-Global algorithm cannot handle the case when the initial covariance is a diagonal matrix.
\end{itemize}
The first limitation is explained further, discussing the current choice of $\lambda$ from the three-step Gibbs sampler. It suggests that using the fully conditional posterior distributions of $\lambda^2$ with a gamma prior distribution could lead to an optimal $\lambda$ posterior distribution due to conjugacy. However, obtaining these posterior estimates for $\lambda$ takes a significant amount of execution time, similar to sampling other posterior estimates.

The second limitation is explained by discussing the behavior of the update formula for $\tilde{\Sigma}$ in the univariate Local-Global algorithm. It highlights that if the initial covariance $\tilde{\Sigma}$ is diagonal, it remains a diagonal matrix throughout the iterations, which restricts the algorithm's generalizability. The explanation provides an example of how the update formula results in a diagonal matrix, causing a discrepancy when the actual $\Sigma$ is non-diagonal.

\section{Future Work}
Several improvements can be explored as a future study to address the aforementioned limitations.
\begin{itemize}
	\item Propose a Bivariate-Local-Global Algorithm to address the problem when the initial covariance is a diagonal matrix.
	\item Derive the updated formula of $\sigma^2$.
\end{itemize}

Firstly, since the update for each iteration is done via a pair of variables, the covariance matrix of each variable will be updated multiple times for each pair.
Secondly, our Local-Global algorithm can only obtain the posterior distribution for the regression coefficient $\beta$, while the temporary assumption of independence between $\sigma^2$ and $\beta$ is limiting. In fact, it is necessary to derive and explore an alternative update formula to calculate $\Sigma$ in the future.

\section{Conclusion}

In conclusion, we have proposed a novel algorithm for the Bayesian Lasso, which utilizes a local approximation correction approach to capture the correlation between the distribution of the target variable and other variables. The introduction of the Lasso distribution enhances the precision of the local approximation, leading to significant improvements over the MFVB approximation.

Looking ahead, our future work will primarily focus on successfully implementing the Bivariate-Local-Global algorithm as mentioned in the last paragraph in Chapter 3. Furthermore, a valid derivation formula for updating the covariance distribution $q(\sigma^2)$ will be a key area of exploration. These efforts will contribute to advancing the accuracy and efficiency of Bayesian variable selection and approximation methods.

