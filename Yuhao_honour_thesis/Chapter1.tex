\chapter{Introduction}
\label{Chapter1}
\section{Background and Motivation}

% Lasso -> Bayesian Lasso -> Bayesian Lasso Problem -> challenges of Bayesian Lasso -> % Approximate Bayesian Inference(refer to bayesian paradigm) -> Variational Inference -> % motivation

%%%%%%%%%%%%%%%% Next Version
%%%%%%%%%%%%%%%%

%\textbf{Why Bayesian Lasso problem}
%Advantages: 1. Incorporate Variation (inferential quantity), bring prior if we have strong prior, %2. Bayesian: Potential for automatic tuning selection while Freq cross-validation)
%Disadvantages:
%Lasso -> bayesian lasso -> MCMC approximate bayesian inference

\textbf{Introduction of Lasso Problem:}
The Least Absolute Selection and Shrinkage Operator (Lasso) regression proposed by \cite{tibshirani_1996} belongs to the class of shrinkage methods. As one of the traditional shrinkage methods, Lasso regression has been proven very useful in Statistical Community over the years. 

The Lasso serves two purposes. One is the estimation of regression parameters and the second is the effective shrinkage of the coefficients to achieve variable selection. This is a fundamental difference between Lasso with some other shrinkage methods. The Lasso regression is helpful particularly for high-dimensional because of  sparse estimates of coefficients. 

\noindent \textbf{Explain the purpose of Lasso:}
The linear regression model is
\begin{equation}
	\label{eq:LRmodel}
	y = \mu 1_n + X\beta + \epsilon,
\end{equation} 
where $\beta$ is a $p\times 1$ vector of the regression coefficient, y is a $n \times 1$ vector of the response variable, X is a $n \times p$ matrix of the covariate, $\mu$ is a $n\times 1$ vector of the population mean, and $\epsilon$ is the model uncertainty which is distributed normally with mean 0 and variance $\sigma^2$. 

The least squares estimator suggests the sum of the square of the difference between the estimated response variable and the response variable should be used as a loss function as described in \autoref{eq:OLS}
\begin{equation}
\label{eq:OLS}
\widehat{\beta} = \underset{\beta}{\operatorname{argmin}}  (\widetilde{y} - X\beta)^T(\widetilde{y}-X\beta).
\end{equation}
The lasso regression estimates linear regression coefficients through $L_1$- constrained least squares. The regression coefficients are estimated based on the lasso method as follows:
\begin{equation}
	\label{eq:lasso1}
	\widehat{\beta}_{lasso} = \underset{\beta}{\operatorname{argmin}} (\widetilde{y}-X\beta)^T(\widetilde{y}-X\beta) + \lambda ||\beta||_1,
\end{equation}
where $\lambda$ $\geq 0$, and $\widetilde{y} =  y - \overline{y}\textbf{1}_n$. The tuning parameter $\lambda$ controls the strength of the penalty. Larger values of $\lambda$ lead to a more sparse solution for the regression coefficient due to a square constraint set resulting from $L_1$ penalty function, resulting in the implicit variable selection of parameter. This can generate a higher prediction accuracy as well as a more interpretable model since we are able to drop the estimated regression coefficient that are estimated as 0 and state they have a weak effect for prediction according to \cite{tibshirani_1996}.

However, due to the non-existence of derivatives of absolute value of regression coefficient $\beta$, alternative improved algorithms have been purposed and deployed such as least angle regression (LARS), iterative soft-thresholding, subgradient methods, and iteratively reweighted least square (IRLS) by \cite{efron_hastie_johnstone_tibshirani_2004},   \cite{beck_teboulle_2009}, \cite{nan_zhang_shuqing_zeng_2005} and \cite{friedman_hastie_tibshirani_2010}.

\noindent \textbf{Bayesian Lasso}.
One drawback of the ordinary lasso is that it cannot properly capture the variation of inferential quantities. To address this issue, \cite{tibshirani_1996} suggested extending lasso estimation under the Bayesian framework. This can be achieved by assigning an independent and identically distributed Laplacian prior from \autoref{eq:MLELasso} and combining it with the likelihood form in \autoref{eq:LassoPrior} to obtain the posterior mode \autoref{eq:lassolikelihood}, i.e.,
\begin{equation}
	\label{eq:MLELasso}
	\widehat{\beta}_{lasso} = \underset{\beta}{\operatorname{argmax}} \  p(\beta|y,\sigma^2,\tau), \quad \mbox{where} \quad  \tau = \frac{\lambda}{2\sigma^2}
\end{equation}

\noindent with
\begin{equation}
	\label{eq:lassolikelihood}
	p(y |\beta,\sigma^2) = N(X\beta,\sigma^2I_n), \quad \mbox{and}
\end{equation}
\begin{equation}
	\label{eq:LassoPrior}
	p(\beta_j) = \left(\frac{\tau}{2}\right) \exp(-\tau|\beta_j|).
\end{equation}
% Add full models

Unfortunately, no tractable solution existed for fitting the Bayesian Lasso until \cite{park_casella_2008}
who explored the lasso model within the Bayesian setting. The choice of a conditional Laplace prior distribution over the regression coefficient $\beta$ conditioning by standard error $\sigma^2$ is added to the Lasso penalty formulation in the frequentist framework. They show that this ensures the unimodality of the posterior distribution. A three-step Gibbs sampler is proposed to draw samples from the Bayes Lasso posterior distribution, which can be utilized for further inference of parameters of interest.

There are several benefits to using the Bayesian Lasso formulation. Firstly, it is easier to implement than the traditional Lasso, although it is more computationally demanding. Secondly, we can generate Bayesian credible intervals simultaneously for the parameters in a model, allowing for the modeling of uncertainty and guiding variable selection.
Thirdly, \cite{park_casella_2008} also state that the Bayesian Lasso model could be a potential solution for addressing the issue of selecting the tuning parameter $\lambda$. We can achieve this by using the marginal maximum likelihood method along with a suitable hyperprior, such as a gamma prior on the square of the tuning parameter (on $\lambda$ or $\lambda^2$).
This approach could support a stable automatic tuning process of choosing an appropriate tuning parameter $\lambda$ for the Lasso model. This is in contrast to the inefficient $K$-fold cross-validation approach typically used to tune the frequentist Lasso model which is time-consuming and computationally demanding. Lastly, the three-step Bayesian Lasso Gibbs sampler proposed by \cite{park_casella_2008} would yield an exact posterior distribution that can be sampled given the exact forms of the full conditional distributions of each model parameter. 

Theoretically, \cite{khare_hobert_2013} demonstrate a Bayesian Lasso Gibbs sampler version of central limit theorem (CLT) which indicates that Bayesian Lasso Gibbs sampler satisfies geometric ergodicity for any values of sample size $n \geq 3$ for an arbitrary number of regression coefficient $p$, data matrix $X$, tuning parameter $\lambda$. 

% This means the Bayesian lasso Gibbs sampler is able to achieve asymptotically uncertainty of posterior estimation. To address this issue, \cite{FastBL} invented a reduced step Gibbs sampler instead, successfully accelerating the sampling procedure. 
\noindent \textbf{Approxiamte Bayesian Inference:}
A review of Bayesian inference occurs in Section \ref{bayeisanP}, but challenges of Bayesian inference motivate the approximate Bayesian inference method thereafter. To be specific, \cite{bishop_2006} states three main challenges of obtaining a posterior distribution. Firstly, the dimension of the target parameter might be high, which results in heavy computational costs for estimating posterior distribution. Secondly, it takes a long time to converge to the target posterior distribution. Thirdly, the computation of the posterior mean parameter $\int_{\theta} \theta \, p(\theta|\mathcal{D}) d\theta$ 
%has a high chance that it will not have a simple calculation, resulting in the fact that there might 
does not have a closed-form analytical solution for integration. Much effort has been made over the years, there are two main types of sampling approaches that are effective currently, which are stochastic sampling algorithms and deterministic approximation algorithms. 

There are two genres of ABI methods, which include stochastic approaches such as Markov Chain Monte Carlo (MCMC), where an exact result can be obtained if infinite computational resources are assigned. The other category lies in deterministic approaches, which provide a faster substitution compared to stochastic approximation approaches.
As stated above, approximate inference methods such as MCMC are used for posterior distribution estimation. The need for approximate Bayesian inference methods arises from the difficulties and impracticality of estimating the posterior distribution of the model parameters.\\
\noindent \textbf{Challenges of Bayesian Lasso model:}
The three-step Gibbs sampler belongs to the class of MCMC algorithms and is a technique for sampling from a probability distribution by constructing a Markov chain that has the posterior distribution as its equilibrium distribution,  where each variable is sampled in turn given the current values of the other variables. 
The burn-in period in MCMC methods is the initial set of iterations that are discarded before collecting samples with the purpose of ensuring that the measurements are stable and consistent before beginning the actual analysis. The rationale behind this is to reduce the dependence on the initial values and mitigate the impact of starting the chain from a poor position in the state space. The length of the burn-in period can vary based on the specific characteristics of the chain, and determining the appropriate length often involves some level of subjective judgment. 
The three-step Gibbs sampler, however, has several challenges. One of the fundamental challenges is its slow convergence. This issue is more highlighted in high-dimensional problems, and it increases the execution time of the algorithm to reach the stationary distribution.\\
\noindent \textbf{Approximation Algorithms: Deterministic type \& VI:}
As mentioned before, due to the limitations of stochastic algorithms, alternative methods such as deterministic Variational Inference (VI) have become popular due to their fast speed and simple computation.
Numerous algorithms have been designed and utilized widely such as variational Bayes (VB), Expectation Propagation algorithms, etc. A common variation is the Coordinate Ascent Variational Inference approach produced by \cite{Blei2003LDA}, which assumes that the approximation originates from an analytically tractable class of distribution $Q$. Afterward, it attempts to search for the distribution from this family that is closest to the target posterior distribution with some discrepancy metric, such as the Kullback-Leibler (KL) divergence. An optimization-based system in \autoref{eq:VI1} is established by iteratively updating variational parameters with an appropriate optimization algorithm. An easy-to-implement optimization algorithm in this context is Coordinate Ascent which could obtain approximated posterior distribution in the family of $Q$. While the most common choice is the Normal distribution due to its simple form and adaptability to other distributions.
To further illustrate the intuition, Figure \ref{fig:VIoptimization} provides further explanation of the aforementioned intuition, goal, and procedure of Variational Inference.
\begin{figure}[H]
	\center
	\includegraphics[scale = 0.2]{VIoptimization}
	\caption{Variational Inference intuition, where $X$ is data $\mathcal{D}$, $\mathcal{D}$ is equivalent to $Q$ defined above}
	\label{fig:VIoptimization}
\end{figure}
\begin{equation}
	\label{eq:VI1}
	q^{*}(\theta) = \underset{q_{\theta} \in Q}{\operatorname{argmin}} \quad \textnormal{KL}(q(\theta)||p(\theta|\mathcal{D})) \coloneqq \int q(\theta)\log\left(\frac{q(\theta)}{p(\theta|\mathcal{D})}\right)d\theta,
\end{equation}
\noindent where
\begin{equation}
	\label{eq:KL2}
	\textnormal{KL}(q||p(\cdot|\mathcal{D})) = - \int q(\theta)\log\left(\frac{p(\theta)p(\mathcal{D}|\theta)}{q(\theta)}\right) d\theta + \log p(\mathcal{D}).
\end{equation}
In addition, the exact form of $\textnormal{KL}$ divergence can be found in \autoref{eq:KL2}.
In practice, minimizing the KL divergence from \autoref{eq:VI1} is difficult due to its complexity, and therefore it is often converted into an equivalent formulation in \autoref{eq:KL} that maximizes the lower bound of $\log(p(y))$. This lower bound is known as the Evidence Lower Bound (ELBO) and can be defined by \autoref{eq:KL}.
\begin{equation}
	\label{eq:KL}
	q^*(\theta) = \underset{q_{\theta} \in Q}{\operatorname{ argmax}} \textnormal{ ELBO}(q(\theta)).
\end{equation}
\noindent Note that
\begin{equation}
	\label{eq:ELBO}	
	\begin{aligned}
		\textnormal{ELBO}(q(\theta)) &= \int q(\theta)\log\left( \frac{p(\theta)p(\mathcal{D}|\theta)}{q(\theta)} \right) d\theta = \mathop{\mathbb{E}_{q(\theta)}}\log\left( \frac{p(\theta)p(\mathcal{D}|\theta)}{q(\theta)} \right) \\
		& = \mathop{\mathbb{E}_{q(\theta)}}[\textnormal{log}p(\theta,\mathcal{D})]
		- \mathop{\mathbb{E}_{q(\theta)}}[\textnormal{log}q(\theta)]\\
		&= \mathop{\mathbb{E}_{q(\theta)}}[\textnormal{log}p(\theta,\mathcal{D})]
		- \textnormal{KL}(q(\theta)||p(\theta)).
	\end{aligned}
\end{equation}
\textbf{Mean Field Variational Bayes:}
% Change reference
The most common Variational Inference algorithm is known as Mean Field Variational Bayes (MFVB) motivated by mean-field theory in statistical physics, as proposed by 
\cite{parisi1988statistical}. The algorithm assumes that the approximated distribution is
a product of independent parameter distributions from set $Q$, as described in \autoref{eq:MFVBassume}, assuming there are $k$ sub-parameters of the parameter $\theta$, i.e.,
\begin{equation}
	\label{eq:MFVBassume}
	q(\theta) = \prod_{i=1}^{k} q_i(\theta).
\end{equation}

MFVB has been adapted and developed over the last three decades, where it has been used in mixture modeling, probabilistic graphical modeling, and variable selection. We will introduce more about the algebra of MFVB in Chapter \ref{Chapter2}. \\
\textbf{Advantages of MFVB:}
As mentioned above, one advantage of the MFVB is that it has a lower computation cost and so scales well with the dimensionality of the data, making it a desirable choice for high-dimensional datasets. For instance, if there exist a billion images that require to be fitted into a probabilistic machine learning model, then exact methods such as MCMC will be computationally demanding, while Variational Inference would have a higher chance to sacrifice accuracy with hundreds of times faster speed.
%Another important factor is that distributed computation and stochastic optimization technique can be embedded in the Variational Inference framework due to its nature of an optimization system, while the Variational Inference originates from Machine Learning for approximating probability density for complex probabilistic graphical model \cite{jordan_ghahramani_jaakkola_saul_1999}. 
Secondly, the time-efficiency of MFVB becomes another significant factor why it is popular, given the fact it only involves updating variational parameters iteratively until convergence, as opposed to MCMC which produces correlated samples that limit the ideal behavior of the MCMC algorithm.\\
\textbf{Drawbacks of MFVB:}
Disadvantages of MFVB include inexact approximation results under some scenarios, although it could capture some marginal density information. For example, it is suggested by \cite{blei_kucukelbir_mcauliffe_2017},
%Might have problem with citation pRML%
that the Variational Inference algorithm might underestimate the covariance between the parameters of interest if the inter-parameter correlation is strong. It tends to ignore the correlation between parameters, resulting in unideal behavior as a result. Figure \ref{fig:VIdemo}
demonstrates this phenomenon, the true overall posterior of $x_2$ and $x_1$ are correlated with an eclipse-shaped density, while a circled-shape mean-field approximation is established instead due to its product density family specification.
\begin{figure}
	\includegraphics[width=\linewidth]{VIdemo}
	\caption{Visualization of Mean-Field Variational Approximation compared with exact posterior when the correlation is large}
	\label{fig:VIdemo}
\end{figure}
We will expand properties and derivation of Variational Inference more in subsection \ref{VI}.\\
Overall, Variational Inference has proven its effectiveness in distinct application fields such as speech recognition and document retrieval in natural language processing, computer vision, etc. Despite the small disadvantages of Variational Inference, the potential of variational approximation has not been fully explored by researchers, its ability to provide a reliable posterior estimate is invaluable in the future in the era of big data and deep learning nowadays.\\
\textbf{Motivation:}
This study is motivated by the desire to improve the approximation accuracy of Variational Bayes and to take advantage of the properties of Bayesian Lasso regression coefficient estimation for variable selection and standard error estimation. There is an increasing demand for fast approximate inference. We would like to design new VI-based algorithms for the Bayesian Lasso regression problem, for the purpose of obtaining Bayesian Lasso posterior distribution in a much faster and more accurate manner.

The aim of our approach is to use the posterior mean $\mu$ and covariance $\Sigma$ for the regression coefficient $\beta_j$ for the $j_{th}$ variable and use these to form an improved approximation for the marginal distributions. These approximated marginal distributions have the form of a novel distribution, a univariate Lasso distribution.
We then use the mean and covariance of the Lasso distribtion to improve the VB Gaussian approximation of the posterior distribution for $\beta$. We have demonstrated that our algorithm's approximation accuracy surpasses that of several existing algorithms, including MFVBs. Even though the speed of our algorithm is slightly slower than MFVB, the approximation accuracy illustrates a small gap between exact estimation from MCMC, with a hundred times faster time complexity. 

Nevertheless, the drawback to this method is that if the initial global covariance matrix is diagonal it will remain diagonal after it is updated.

To remedy this issue, we have also purposed another algorithm based on marginal likelihood estimation by a bivariate Lasso distribution. Instead of updating the corresponding mean, and covariance matrix for each variable in each iteration, the marginal likelihood of each pair of variables would be matched, so that further generalize our algorithm. Our conclusions are the univariate Lasso algorithm is faster with a lower accuracy while the bivariate Lasso algorithm is slower with a higher accuracy since it updates each pair of variables at a time resulting in ${p\choose 2}$ of unique pairs. We will show the full intuition and idea later in Chapter \ref{Chapter3}.
By utilizing and fitting both univariate and bivariate Lasso distribution to each of the marginal distributions, an improved estimate for global Gaussian approximation can be obtained as defined in \autoref{eq:GV}. 

%Our contributions have been listed in the following subsection \ref{cont}.
\begin{equation}
	\label{eq:GV}
	q^{*}(\theta) \approx N(\mu^*,\Sigma^*)
\end{equation}

\noindent 
Finally, we will show our experiment result in Chapter \ref{Chapter4} using various accuracy metrics.

\section{Contributions}
\label{cont}
Our main contributions could be summarised as:
\begin{itemize}
	\item Introduction of univariate and bivariate Lasso distributions.
 
	\item Derivation of properties for univariate Lasso distribution, such as the expectation, variance, cumulative density function, etc.
 
	\item Derivation of properties for bivariate Lasso distribution such as the expectation, variance, cumulative density function, etc.
 
	\item Implementation of univariate and bivariate Lasso distributions in R.
 
	\item Design of two new VI approaches based on local approximation via the univariate Lasso distribution and bivariate Lasso distribution respectively.
 
	\item Conduct of experiment to test the two algorithms under
 several benchmark datasets using several evaluation metrics for approximation accuracy.
\end{itemize}



\section{Thesis Organization}
The thesis is organized as follows. Chapter 1 briefly illustrates the motivation and background of the Lasso problem, Bayesian Lasso Problem, and Approximate Bayesian Inference, with a specific focus on deterministic variational approximations. Chapter 2  briefly reviews and explains the details of the methods in previous work such as the Lasso problem, Approximate Bayesian Inference algorithm, MCMC, Bayesian Expectation Maximization algorithm and their variants, and MFVB. We present our main methodology of the variational algorithm in Chapter 3, followed by a comprehensive experiment for testing the effectiveness of the algorithm in Chapter 4. Chapter 5 is allocated for discussion and conclusion. 



