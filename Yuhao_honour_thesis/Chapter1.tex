\chapter{Introduction}
\label{Chapter1}
\section{Background and Motivation}

%%%%%%%%%%%%%%%% Next Version
%%%%%%%%%%%%%%%%

%\textbf{Why Bayesian Lasso problem}
%Advantages: 1. Incorporate Variation (inferential quantity), bring prior if we haev strong prior , %2. Bayesian: Potential for auotomatic tuning selection while Freq cross validation)
%Disadvantages:
%Lasso -> bayesian lasso -> MCMC approximate bayesian inference

\textbf{Introduction of Lasso Problem}\\
The Least Absolute Shrinkage Operator(Lasso) regression proposed by \cite{tibshirani_1996} belongs to one of the shrinkage methods, the main idea behind shrinkage method is that it will eliminate regression coefficient that are close to zero, by discarding the subset of them, the rest of the model might shares numerous advantages including interpretability and less prediction error than the model fitting by all predictors. As one of the most traditional shrinkage methods, Lasso regression has been proven for his success in Statistical Community over the years. It has also been deployed in Machine Learning Community as well, as another name called $L_1$ regularization techniques for effectively avoiding over-fitting problems and reducing model complexity. The main idea of lasso is adding a penalty term of absolute value of coefficients in addition to the sum of squared value of residuals. Due to squared constraint set shape of regression coefficient, lasso penalty regression will tend to have a higher chance to shrink the subset of the estimated regression coefficient to be zero, generating a more interprable model if discard them. \cite{tibshirani_1996} has also shown that the effect of new submodel is competitive as opposed to other typical shrinkage methods such as subset selection and ridge regression. \\
Moreover, he also purpose that the lasso estimate can also be calculated under the Bayesian framework, if independent and identically distributed laplacian prior is assigned to the problem.\\


\textbf{Introduction of Bayesian Lasso Problem(Why Bayesian Lasso)}
Furthermore, \cite{park_casella_2008} further explore the Lasso problem under the setting of Bayesian framework, where the choice of a conditional Laplace prior distribution over the regression coefficietn $\beta$ given by standard error $\sigma^2$ is equivalent to the Lasso penalty formulation in the frequentist framework. It could also support a more stable automatic tuning process of choosing the most appropriate tuning parameter $\lambda$ for the Lasso problem. 






\textbf{Challenges of Bayesian Lasso Problem}

\textbf{The use of Variational Inference}





\textbf{Approximation Algorithm: Stochastic type}
The most typical stochastic approximation algorithm is Markov Chain Monte Carlo(MCMC), where an exact posterior distribution can be sampled given form of conditional distribution of each model parameter conditioning on rest of the other parameters. Theoretically, MCMC will lead to an arbitary precise result of posterior distribution approximation if an arbitarily long burn-in period is allocated. However, MCMC is notorious for suffering from long execution time especially and heavy computational cost, and we will expand more the property in the following subsection \ref{MCMC}.

\textbf{Approximation Algorithm: Deterministic type}
On the other hand, determinstic approach has also arised as an faster substitution compared with stochastic approximation approaches. Numerous algorithms have been designed and utilized widely such as Variational Bayes, Expectation Propagation algorithms etc. Determinstic approaches assume the approximation originates from a tractable distribution first and attempt to search for the distribution from this family that is the closest to the target posterior distribution by optimization techniques, it has been indicated that Variational Inference algorithm demonstrate a descent computation cost and time-efficiency. 

\textbf{Variational Bayes}
The most traditional Variational Inference algorithm is known as Mean-Field Variational Bayes motivated by mean-field Theory in statistical physics yielded by 
\cite{jordan_ghahramani_jaakkola_saul_1998} and \cite{attias_1999}, which assume the approximated distribution is from independent product of parameter distribution.
Meanwhile, disadvantages of Variational Bayes include inexact approximation result under some scenarios. For example, it is suggested by \cite{bishop_2006} that Variational Inference algorithm might underestimate the covariance between parameter of interest, if parameter of interest have a strong correlation. We will expand properties and derivation of Variational Inference more in subsection \ref{VI}.\\

\textbf{Motivation}
Motivated by the intention of further enhancing the approximation accuracy of Variational Bayes, we have designed two new Variational algorithms, particularly for Bayesian Lasso problem. By utilizing and fitting a Lasso distribution to marginal distribution, an improved estimate for global Gaussian Approximation can be obtained. Our contribution have been listed in the following subsection \ref{cont}.\\



\section{Contribution}
\label{cont}
Our main contribution could be concluded as the following part:
\begin{itemize}
	\item Introduction of Lasso Distribution
	\item Derivation of properties for Univariate Lasso Distribution.
	\item Derivation of properties for Multivariate Lasso Distribution.
	\item Implementation of Univariate Lasso Distribution and Multivariate Lasso Distribution property in R.
	\item Design of two new Variational Inference approaches based on local approximation by univariate lasso distribution and multivariate lasso distribution respectively.
	\item Conduct of experiment to testify two algorithms under  dataset by several evaluation metrics for approximation accuracy such as .
\end{itemize}



\section{Thesis Organization}
This paper will be divided up into 5 chapters. Chapter 1 briefly illustrate the motivation and background of the Lasso problem, Bayesian Lasso Problem and fast Approximate Bayesian Inference such as Variational Approximation. Chapter 2 will briefly review and explain the details of the methodlogy in previous work such as the lasso problem, MCMC(Monte Carlo Method) and their variants and Mean-Field Variational Bayes(MFVB). We will present our main methodlogy of variational correction algorithm in Chapter 3, followed by a comprehensive experiment for testing the effectiveness of algorithm in Chapter 4. In Chapter 5, we will briefly discuss and explain our result and potential improvement in the future. 



 