\chapter{Introduction}
\label{Chapter1}
\section{Background and Motivation}

%%%%%%%%%%%%%%%% Next Version
%%%%%%%%%%%%%%%%

\textbf{Why Bayesian Lasso problem}
Advantages: 1. Incorporate Variation (inferential quantity), bring prior if we haev strong prior , 2. Bayesian: Potential for auotomatic tuning selection while Freq cross validation)
Disadvantages:
Lasso -> bayesian lasso -> MCMC approximate bayesian inference

\textbf{Introduction of Lasso Problem}\\
The Least Absolute Shrinkage Operator(Lasso) purposed by \cite{tibshirani_1996} belongs to one of the shrinkage methods, the main idea behind shrinkage method is that the method will eliminate regression coefficient that are close to zero, by discarding the subset of them, the rest of the model shares numerous advantages including interpretability and low prediction error than the model fitting by all predictors. As one of the most traditional shrinkage methods, Lasso regression has been proven for his success in Statistical Community over the years. It has also been deployed in Machine Learning Community as well, as another name called $L_1$ regularization techniques for effectively avoiding over-fitting problems and reducing model complexity. The main idea of lasso is adding a penalty term in addition to the sum of absolute value of residuals, for further encouraging the minimization of regression coefficient to be zero.



\textbf{Introduction of Bayesian Lasso Problem}


\textbf{Challenges of Bayesian Lasso Problem}

\textbf{The use of Variational Inference}





\textbf{Approximation Algorithm: Stochastic type}
The most typical stochastic approximation algorithm is Markov Chain Monte Carlo(MCMC), where an exact posterior distribution can be sampled given form of conditional distribution of each model parameter conditioning on rest of the other parameters. Theoretically, MCMC will lead to an arbitary precise result of posterior distribution approximation if an arbitarily long burn-in period is allocated. However, MCMC is notorious for suffering from long execution time especially and heavy computational cost, and we will expand more the property in the following subsection \ref{MCMC}.

\textbf{Approximation Algorithm: Deterministic type}
On the other hand, determinstic approach has also arised as an faster substitution compared with stochastic approximation approaches. Numerous algorithms have been designed and utilized widely such as Variational Bayes, Expectation Propagation algorithms etc. Determinstic approaches assume the approximation originates from a tractable distribution first and attempt to search for the distribution from this family that is the closest to the target posterior distribution by optimization techniques, it has been indicated that Variational Inference algorithm demonstrate a descent computation cost and time-efficiency. 

\textbf{Variational Bayes}
The most traditional Variational Inference algorithm is known as Mean-Field Variational Bayes motivated by mean-field Theory in statistical physics yielded by 
\cite{jordan_ghahramani_jaakkola_saul_1998} and \cite{attias_1999}, which assume the approximated distribution is from independent product of parameter distribution.
Meanwhile, disadvantages of Variational Bayes include inexact approximation result under some scenarios. For example, it is suggested by \cite{bishop_2006} that Variational Inference algorithm might underestimate the covariance between parameter of interest, if parameter of interest have a strong correlation. We will expand properties and derivation of Variational Inference more in subsection \ref{VI}.\\

\textbf{Motivation}
Motivated by the intention of further enhancing the approximation accuracy of Variational Bayes, we have designed two new Variational algorithms, particularly for Bayesian Lasso problem. By utilizing and fitting a Lasso distribution to marginal distribution, an improved estimate for global Gaussian Approximation can be obtained. Our contribution have been listed in the following subsection \ref{cont}.\\



\section{Contribution}
\label{cont}
Our main contribution could be concluded as the following part:
\begin{itemize}
	\item Introduction of Lasso Distribution
	\item Derivation of properties for Univariate Lasso Distribution.
	\item Derivation of properties for Multivariate Lasso Distribution.
	\item Implementation of Univariate Lasso Distribution and Multivariate Lasso Distribution property in R.
	\item Design of two new Variational Inference approaches based on local approximation by univariate lasso distribution and multivariate lasso distribution respectively.
	\item Conduct of experiment to testify two algorithms under  dataset by several evaluation metrics for approximation accuracy such as .
\end{itemize}



\section{Thesis Organization}
This paper will be divided up into 5 chapters. Chapter 1 will briefly illustrate the motivation and background of Variational Approximation. Chapter 2 will briefly introduce basic definition and methodlogy in previous work such as Least Absolute Shrinkage and Selection Operator problem, MCMC(Monte Carlo Method) and their variants and Mean-Field Variational Bayes(MFVB). We will present our main methodlogy of variational correction algorithm in Chapter 3, followed by a comprehensive experiment for testing the effectiveness of algorithm in Chapter 4. In Chapter 5, we will briefly discuss and explain our result and potential improvement in the future. 



 