\chapter{Introduction}
\label{Chapter1}
\section{Background and Motivation}

%%%%%%%%%%%%%%%% Next Version
%%%%%%%%%%%%%%%%

%\textbf{Why Bayesian Lasso problem}
%Advantages: 1. Incorporate Variation (inferential quantity), bring prior if we haev strong prior , %2. Bayesian: Potential for auotomatic tuning selection while Freq cross validation)
%Disadvantages:
%Lasso -> bayesian lasso -> MCMC approximate bayesian inference

\textbf{Introduction of Lasso Problem}\\
The Least Absolute Selection and Shrinkage Operator(Lasso) regression proposed by \cite{tibshirani_1996} belongs to one of the shrinkage methods. As one of the most traditional shrinkage methods, Lasso regression has been proven for his success in Statistical Community over the years.

The Lasso serves as two purposes, one is the estimation of regression parameter, the other is to effective shrinking of the coefficients to achieve variable selection purpose, which is also the fundamental difference of Lasso with other methods. 

\textbf{Explain purpose of Lasso}
The definition of linear regression model of interest can be referred based on the following definition defined by \cite{tibshirani_1996}: the $n \times 1$ vector of regression coefficient $\beta$,  $y$ is the response variable with a dimension of $n \time 1$, $X$ is data matrix after standardization with a dimension of $n \times p$ , $\mu$ is the population mean with a dimension of $n \times 1$, $\epsilon$ is independent and identically distributed normal noise with expectation of 0 and variance of $\sigma^2$. Then the linear model can be explained by the Equation \ref{eq:LRmodel}
\begin{equation}
	\label{eq:LRmodel}
	y = \mu 1_n + X\beta + \epsilon.
\end{equation} 

 The Lasso estimate of regression coefficient is based on Equation (\ref{eq:lasso1}), where the main distinction of Lasso is adding a penalty term of absolute value of regression coefficients $\beta$ in addition to the sum of squared value of residuals from ordinary regression objective function. The value of tuning parameter $\lambda$ is served as a measure of the extent of penalization.
 \begin{equation}
 	\label{eq:lasso1}
 	\hat{\beta}_{lasso} = \underset{\beta}{\operatorname{argmin}} ||\tilde{y}-X\beta||_2 + \lambda ||\beta||_1, \lambda \geq 0. \tilde{y} =  y - \bar{y}\textbf{1}_n \\
 \end{equation}
  
 Larger penalization leads to more sparse solution of regression coefficient due to a square constraint set results from $L_1$ penalty function, so that achieve variable selection of parameter, generating a higher prediction accuracy as well as interpretable model since we could drop the estimated regression coefficient that has 0 and state they have weak effect for prediction according to \cite{tibshirani_1996}.
 
\textbf{Bayesian Lasso}\\
Based on the ordinary Lasso setting, \cite{tibshirani_1996} also suggests that the lasso estimate can also be extended under the Bayesian framework, if independent and identically distributed laplacian prior is assigned to the Lasso model from Equation (\ref{eq:LassoPrior}).

\begin{equation}
	\label{eq:LassoPrior}
	f(\beta_j) = \frac{1}{2\tau} exp(\frac{-|\beta_j|}{\tau}), \tau = \frac{1}{\lambda}
\end{equation}


\textbf{Introduction of Bayesian Lasso Problem(Why Bayesian Lasso)}
Furthermore, \cite{park_casella_2008} further explore the Lasso model under the setting of Bayesian framework, where the choice of a conditional Laplace prior distribution over the regression coefficient $\beta$ given by standard error $\sigma^2$ is added to the Lasso penalty formulation in the frequentist framework. 
\begin{equation}
	\label{eq:lassoprior}
	\pi(\beta |\sigma^2) = \prod_{j=1}^p \frac{\lambda}{2\sqrt{\sigma^2}} e^{-\lambda|\beta_j|/\sqrt{\sigma^2}}
\end{equation}
There are several benefits for using the Bayesian Lasso model, firstly, it has easier implementation than the traditional Lasso. Secondly a Bayesian credible interval of parameter can be utilized for modelling uncertainty and therefore can also guide variable selection based on the interpretation that lasso estimated is regarded as the mode of posterior distribution of $\beta$. \\

%%%%%%%%%%%%%%%%%%%%% To be continued










%% 1: Adapt a prior distribution, 2: automatic tuning process

Secondly, \cite{park_casella_2008} also state that the Bayesian Lasso model could also be a potential solution for addressing the issue of getting optimal tuning parameter $\lambda$. It could support a more stable automatic tuning process of choosing the most appropriate tuning parameter $\lambda$ for the Lasso problem, as opposed to inefficient $K$-fold cross validation approach under the ordinary Lasso model that is time consuming and computational demanding.

\textbf{Challenges of Bayesian Lasso Problem}
However, the traditional way of obtaining posterior distribution of the Bayesian Lasso model such as the  Gibbs sampler from Markov Chain Monte Carlo(MCMC) under the class of stochastic approximation algorithm is time-consuming and computational challenging, even though where MCMC would yield an exact posterior distribution that can be sampled given exact form of conditional distribution of each model parameter conditioning on rest of the other parameters. Theoretically, Gibbs sampler will lead to an arbitrarily precise result of posterior distribution approximation if an arbitrarily long burn-in period is allocated. 


\textbf{Approximation Algorithm: Deterministic type}
On the other hand, determinstic approach has also arised as an faster substitution compared with stochastic approximation approaches. Numerous algorithms have been designed and utilized widely such as Variational Bayes, Expectation Propagation algorithms etc. Determinstic approaches assume the approximation originates from a tractable distribution first and attempt to search for the distribution from this family that is the closest to the target posterior distribution by optimization techniques, it has been indicated that Variational Inference algorithm demonstrate a descent computation cost and time-efficiency. 

\textbf{Variational Bayes}
The most traditional Variational Inference algorithm is known as Mean-Field Variational Bayes motivated by mean-field Theory in statistical physics yielded by 
\cite{jordan_ghahramani_jaakkola_saul_1998} and \cite{attias_1999}, which assume the approximated distribution is from independent product of parameter distribution.
Meanwhile, disadvantages of Variational Bayes include inexact approximation result under some scenarios. For example, it is suggested by \cite{bishop_2006} that Variational Inference algorithm might underestimate the covariance between parameter of interest, if parameter of interest have a strong correlation. We will expand properties and derivation of Variational Inference more in subsection \ref{VI}.\\

\textbf{Motivation}
Motivated by the intention of further enhancing the approximation accuracy of Variational Bayes, we have designed two new Variational algorithms, particularly for Bayesian Lasso problem. By utilizing and fitting a Lasso distribution to marginal distribution, an improved estimate for global Gaussian Approximation can be obtained. Our contribution have been listed in the following subsection \ref{cont}.\\



\section{Contribution}
\label{cont}
Our main contribution could be concluded as the following part:
\begin{itemize}
	\item Introduction of Lasso Distribution
	\item Derivation of properties for Univariate Lasso Distribution.
	\item Derivation of properties for Multivariate Lasso Distribution.
	\item Implementation of Univariate Lasso Distribution and Multivariate Lasso Distribution property in R.
	\item Design of two new Variational Inference approaches based on local approximation by univariate lasso distribution and multivariate lasso distribution respectively.
	\item Conduct of experiment to testify two algorithms under  dataset by several evaluation metrics for approximation accuracy such as .
\end{itemize}



\section{Thesis Organization}
This paper will be divided up into 5 chapters. Chapter 1 briefly illustrate the motivation and background of the Lasso problem, Bayesian Lasso Problem and fast Approximate Bayesian Inference such as Variational Approximation. Chapter 2 will briefly review and explain the details of the methodlogy in previous work such as the lasso problem, MCMC(Monte Carlo Method) and their variants and Mean-Field Variational Bayes(MFVB). We will present our main methodlogy of variational correction algorithm in Chapter 3, followed by a comprehensive experiment for testing the effectiveness of algorithm in Chapter 4. In Chapter 5, we will briefly discuss and explain our result and potential improvement in the future. 



 