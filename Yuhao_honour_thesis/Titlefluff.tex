\begin{titlepage}
\centering
\topskip0pt
\vspace*{\fill}
\huge{\textbf{{\bf\Huge A novel algorithm for the Bayesian Lasso: A local approximation adjustment approach}}}\\


%% A Statistical Model for Cell Reprogramming?
%% A novel algorithm to model cell reprogramming?
%% A novel model for cell reprogramming using matched scRNA-seq and scATAC-seq data?

\vspace{2.5cm}
\LARGE{Yuhao Li}\\
\vspace{1cm}
\large{Supervisor: A/Prof. John Ormerod and \\ Dr. Mohammad Javad Davouddabadi}\\
\large{A thesis submitted in partial fulfillment of \\ the requirements for the degree of \\  Bachelor of Science(Honour)(Data Science)} \\
\vspace{1cm}
\large{School of Mathematics and Statistics}\\
\vspace{1.5cm}
\date{September 2021}
    \includegraphics[scale=0.75]{UsydLogo.pdf}\\
\vspace{1.5cm}
\large{June 2023}\\
\vspace*{\fill}

\thispagestyle{empty}
\end{titlepage}

\pagenumbering{roman}


\pagebreak
\hspace{0pt}
\begin{center}
    \textbf{\large Statement of originality}\\
    \vspace{0.5cm}
\end{center}

\noindent This is to certify that to the best of my knowledge, the content of this thesis is my own work. This thesis has not been submitted for any degree or other purposes.\\
\\
\noindent I certify that the intellectual content of this thesis is the product of my own work and that all the assistance received in preparing this thesis and sources have been acknowledged.\\
\\
\\
\\
Yuhao Li

\pagebreak
\hspace{0pt}

\begin{center}
    \textbf{\large Abstract }\\
    \vspace{0.5cm}
\end{center}
Least Absolute Shrinkage and Selection Operator penalized regression, with an abbreviation of Lasso penalized regression, is regarded as a core statistical technique for simultaneous coefficient estimation and model selection. The idea of Lasso is to add the additional $l_1$ norm penalty function to the objective function that have sum of squared residual only for ordinary regression, generating and eliminating sparse coefficient to achieve efficient and interpretable model selection.

By considering the Lasso problem from a Bayesian perspective, the Bayesian Lasso model uses a double-exponential prior to model the variation of inferential quantity and to obtain interval estimates of coefficients. Moreover, the Bayesian framework offers an automatic tuning process for the tuning parameter $\lambda$, which controls the strength of penalization. This process replaces the time-consuming n-fold cross-validation technique used in the ordinary Lasso.

On the other hand, obtaining the posterior of the Bayesian Lasso model involves using Monte Carlo Markov Chain methods, such as Gibbs sampler for exact posterior distribution. Even though MCMC is famous for its oracle property such as generating exact target samples if Markov Chain is run for enough iterations, it is slow and has a high computational cost. 

Meanwhile, variational approximation, as a deterministic approximation algorithm for intractable posterior distribution, has been applied prevalently for fast Approximate Bayesian Inference(ABI) among the Bayesian Statistical community. It is also a faster alternative to Monte Carlo methods such as MCMC.

The concept of variational approximation involves proposing a set of known densities, and subsequently identifying the density within that set that best approximates the target. The level of approximation is evaluated using Kullback-Leibler (KL) divergence.

Nevertheless, variational approximation unlike the MCMC methods does not guarantee to approximate the exact target density. It can only find a density close to the target which means the approximation accuracy might  be a pivotal concern and it fails to provide reliable estimates of posterior variances

In order to address the slow speed issue of obtaining posterior distribution of the Bayesian Lasso problem, new alternative ABI methods would be explored, especially for deterministic algorithm such as variational Bayes.

This thesis presents two recently discovered distributions, the univariate and multivariate lasso distributions. These distributions can be used to assist in approximating marginal distributions. Additionally, the thesis introduces two highly efficient and precise variational approximation algorithms and their application in solving the Bayesian Lasso regression problem.


The first method involves matching with a marginal univariate lasso distribution by updating the global parameter for each variable per iteration. Additionally, we propose another algorithm that matches a local bivariate lasso distribution to update the global parameter for each pair of variables per iteration. This algorithm successfully addresses the issue that arise when an initial diagonal covariance matrix is assigned.

To verify the efficiency and accuracy of our algorithm, we conducted numerous experiments using real-world datasets such as the Hitters Dataset and evaluated it using several metrics such as $l_1$ accuracy and running speed.


\newpage

\begin{center}
    \textbf{\large Acknowledgements}\\
    \vspace{0.5cm}
\end{center}
	Firstly, I would like to express my deep thanks to my supervisor: A/Prof: John Ormerod and Co-supervisor Dr. Mohammad Javad Davoudabadi, for their constant guidance and patience throughout this year. I would not make it so far without them.
   	Secondly, I would like to thank my friends, my family, my mother, father. The entire honour year has been full of challenges not only in the researching a novel area, but also the heavy coursework workload. I could not have overcome them without your support behind the scenes.
    Finally, I would like to thank the University of Sydney, for providing me a chance to study and research in this school. It is an unforgettable experience.
\vfill
\hspace{0pt}
\pagebreak
\newpage