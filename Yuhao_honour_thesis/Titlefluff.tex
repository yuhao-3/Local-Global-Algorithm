\begin{titlepage}
\centering
\topskip0pt
\vspace*{\fill}
\huge{\textbf{{\bf\Huge A novel algorithm for the Bayesian Lasso: A local approximation adjustment approach}}}\\


%% A Statistical Model for Cell Reprogramming?
%% A novel algorithm to model cell reprogramming?
%% A novel model for cell reprogramming using matched scRNA-seq and scATAC-seq data?

\vspace{2.5cm}
\LARGE{Yuhao Li}\\
\vspace{1cm}
\large{Supervisor: A/Prof. John Ormerod and \\ Dr. Mohammad Javad Davoudabadi}\\
\large{A thesis submitted in partial fulfillment of \\ the requirements for the degree of \\  Bachelor of Science(Honour)(Data Science)} \\
\vspace{1cm}
\large{School of Mathematics and Statistics}\\
\vspace{1.5cm}
\date{September 2021}
    \includegraphics[scale=0.75]{UsydLogo.pdf}\\
\vspace{1.5cm}
\large{June 2023}\\
\vspace*{\fill}

\thispagestyle{empty}
\end{titlepage}

\pagenumbering{roman}


\pagebreak
\hspace{0pt}
\begin{center}
    \textbf{\large Statement of originality}\\
    \vspace{0.5cm}
\end{center}

\noindent This is to certify that to the best of my knowledge, the content of this thesis is my own work. This thesis has not been submitted for any degree or other purposes.\\
\\
\noindent I certify that the intellectual content of this thesis is the product of my own work and that all the assistance received in preparing this thesis and sources have been acknowledged.\\
\\
\\
\\
Yuhao Li

\pagebreak
\hspace{0pt}

\begin{center}
    \textbf{\large Abstract }\\
    \vspace{0.5cm}
\end{center}
Least Absolute Shrinkage and Selection Operator penalized regression, with an abbreviation of Lasso penalized regression, is regarded as a core statistical technique for simultaneous coefficient estimation and model selection. The idea of Lasso is to add the additional $l_1$ norm penalty function to the objective function that has sum of squared residual only for ordinary regression, generating and eliminating sparse coefficient estimates to achieve efficient and interpretable model selection.

By considering the Lasso problem from a Bayesian perspective, the Bayesian Lasso model uses a double-exponential prior to model the variation of inferential quantity and to obtain interval estimates of coefficients. Moreover, the Bayesian framework offers an automatic tuning process for the tuning parameter $\lambda$, which controls the strength of penalization. This process replaces the time-consuming $n$-fold cross-validation technique used in the ordinary Lasso.

On the other hand, obtaining the posterior of the Bayesian Lasso model involves using Monte Carlo Markov Chain methods, such as Gibbs sampler for exact posterior distribution. Even though MCMC is famous for being able to generate 
samples from the posterior distribution if the Markov Chain is run for enough iterations, it is slow and has a high computational cost. 

Meanwhile, variational approximations, as a deterministic class of approximation algorithms for intractable posterior distributions, have been applied prevalently for fast Approximate Bayesian Inference (ABI) in the Bayesian Statistical community. It is also a faster alternative to Monte Carlo methods such as MCMC.

The concept of variational approximation involves proposing a set of known distributional forms, and subsequently identifying the density within that set that best approximates the target. The level of approximation is determined  via the Kullback-Leibler (KL) divergence.

Nevertheless, variational approximation unlike the MCMC methods is  not guaranteed to be exact. It can only find a density close to the target which means the approximation accuracy might be a concern when it fails to provide reliable estimates of posterior variances.\\
In order to address the slow speed of obtaining the posterior distribution for the Bayesian Lasso model, new alternative ABI methods need to be explored, especially for deterministic algorithms such as for variational Bayes and their variants.

This thesis presents two novel distributions, the univariate and bivariate Lasso distributions. These distributions can be used to assist in approximating marginal posterior distributions. In addition, this thesis introduces two highly efficient and precise variational approximation-based algorithms that we apply to solve the Bayesian Lasso regression problem.

The first method involves matching with a marginal univariate lasso distribution by updating a global parameter for each variable per iteration. In addition, we propose another algorithm that matches a local bivariate Lasso distribution to update the global parameter for each pair of variables per iteration. This algorithm successfully addresses the issue that arises when an initial diagonal covariance matrix is assigned.

To verify the efficiency and accuracy of our algorithm, we conducted numerous experiments using benchmark datasets and evaluated it using several metrics such as $l_1$ accuracy and running speed.

\newpage

\begin{center}
    \textbf{\large Acknowledgements}\\
    \vspace{0.5cm}
\end{center}
	Firstly, I would like to express my deep thanks to my supervisor: A/Prof: John Ormerod and Co-supervisor Dr. Mohammad Javad Davoudabadi, for their constant guidance and patience throughout this year. I would not make it so far without them.
   Secondly, I would like to thank my friends, my family, my mother, father. The entire honor year has been full of challenges not only in researching a novel area but also in the heavy coursework workload. I could not have overcome them without your support behind the scenes.
    Finally, I would like to thank the University of Sydney, for providing me a chance to study and research in this school. It is an unforgettable experience.
\vfill
\hspace{0pt}
\pagebreak
\newpage