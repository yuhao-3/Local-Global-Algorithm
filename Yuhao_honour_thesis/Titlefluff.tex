\begin{titlepage}
\centering
\topskip0pt
\vspace*{\fill}
\huge{\textbf{{\bf\Huge A novel algorithm for the Bayesian Lasso: A local approximation adjustment approach}}}\\


%% A Statistical Model for Cell Reprogramming?
%% A novel algorithm to model cell reprogramming?
%% A novel model for cell reprogramming using matched scRNA-seq and scATAC-seq data?

\vspace{2.5cm}
\LARGE{Yuhao Li}\\
\vspace{1cm}
\large{Supervisor: A/Prof. John Ormerod and \\ Dr. Mohammad Javad Davoudabad}\\
\large{A thesis submitted in partial fulfillment of \\ the requirements for the degree of \\  Bachelor of Science(Honour)(Data Science)} \\
\vspace{1cm}
\large{School of Mathematics and Statistics}\\
\vspace{1.5cm}
\date{September 2021}
    \includegraphics[scale=0.75]{UsydLogo.pdf}\\
\vspace{1.5cm}
\large{June 2023}\\
\vspace*{\fill}

\thispagestyle{empty}
\end{titlepage}

\pagenumbering{roman}


\pagebreak
\hspace{0pt}
\begin{center}
    \textbf{\large Statement of originality}\\
    \vspace{0.5cm}
\end{center}

\noindent This is to certify that to the best of my knowledge, the content of this thesis is my own work. This thesis has not been submitted for any degree or other purposes.\\
\\
\noindent I certify that the intellectual content of this thesis is the product of my own work and that all the assistance received in preparing this thesis and sources have been acknowledged.\\
\\
\\
\\
Yuhao Li

\pagebreak
\hspace{0pt}

\begin{center}
    \textbf{\large Abstract }\\
    \vspace{0.5cm}
\end{center}
Least Absolute Shrinkage Operator penalized regression, with an abbreviation of Lasso penalized regression, is regarded as a core statistical technique for simultaneous coefficient estimation and model selection. The idea of Lasso is to add the additional $l_1$ norm penalty function to the objective function that have sum of squared residual only for ordinary regression, generating and eliminating sparse coefficient to achieve efficient and interpretable model selection.

By discovering the Lasso problem from Bayesian perspective, the Bayesian Lasso problem use a double-exponential prior for modelling variation of inferential quantity and obtaining interval estimate of coefficients. In addition, an automatic tuning process of tuning parameter $\lambda$ that control the strength of penalization can also be performed in the Bayesian framework instead of time-consuming $n$-fold cross validation technique under the ordinary Lasso.

On the other hand, obtaining the posterior of the Bayesian Lasso model involves using Monte Carlo Markov Chain method such as Gibbs sampler for exact posterior distribution. Even though MCMC is famous for its oracle property such as generating arbitary exact target samples if burn-in period is enough, MCMC is slow with high computational cost. 

Meanwhile, Variational Approximation: as a deterministic approximation algorithm for intractable posterior distribution, has been applied prevalently for fast Approximate Bayesian Inference(ABI) among the Bayesian Statistical community, while it is also a faster alternative to Monte Carlo methods such as Markov Chain and Monte Carlo(MCMC).

The main idea behind Variational Approximation is: given an assumed distribution set, it will search for an optimal posterior distribution by continuing minimizing the gap between true posterior and estimated posterior such as using Kullbackâ€“Leibler divergence(KL-divergence) as a distance metric.

Nevertheless, elegant property in MCMC such as obatining exact posterior if infinite burn-in time period is assigned, doesn't occur in Variational Inference, which means the approximation accuracy will be a pivotal concern as unsatisfied distribution such as underestimating the variance when the correlation of variables becomes large. 

In order to address the slow speed issue of obtaining posterior distribution of the Bayesian Lasso problem, new alternative Fast Approximate Inference(ABI) methods would be explored, especially for deterministic algorithm such as Variational Bayes.

In this thesis, we will firstly introduce univariate and the multivariate lasso distribution, which are newly discovered distribution that could be matched for aiding marginal distribution approximation, followed by the introduction of two fast and more accurate Variational Approximation algorithms and their application in the Bayesian Lasso regression problem. By assuming the global parameter assuming Gaussian Approximation, the information of local parameter distribution would be accommodated by the lasso distribution so that global approximated distribution would be obtained by product of local distribution and conditional Gaussian distribution.


The first method involves matching with marginal univariate lasso distribution by updating global parameter for each variable per iteration. Additionally, we propose another algorithm for matching a local bivariate lasso distribution for updating global parameter for each pair of variables per iteration, successfully addressing the issue when initial diagonal covariance matrix is assigned.

To verify the efficiency and accuracy of our algorithm, numerous experiments would be conducted under real-world datasets such as Hitters Dataset using several evaluation metric such as $l_1$ accuracy and matrix norm.
Our result suggest their high Variational Approximation accuracy with a descent time efficiency, compared with the traditional Monte Carlo methods and Mean-Field Variational Bayes(MFVB).


\newpage

\begin{center}
    \textbf{\large Acknowledgements}\\
    \vspace{0.5cm}
\end{center}
   Thanks to Supervisor and family 
   % Firstly, I would like to express my deep thanks to my supervisor % 

   Finally, I would like to thank my friends  my family: my mother, father, brother, grandparents, and cousins. The last couple years has been full of adversity and I could not have overcome it without your support behind the scenes.
   
\vfill
\hspace{0pt}
\pagebreak
\newpage