{
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# compute change of log-likelihoods
err <- max(abs(vtheta - vtheta_old))
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 10,a=1,b=2,lambda=lam)
# calculating multivariate normal distribution
library(mvtnorm)
bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
sigma2.curr = sigma2.init
beta.curr = beta.init
A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
# Store incomplete log-likelihoods for each iteration
log_like = c()
log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
delta.ll = 1
err = 1
n.iter = 1
while((err>epsilon)&(n.iter<=max.iter))
{
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# compute change of log-likelihoods
err <- max(abs(vtheta - vtheta_old))
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=lam)
# calculating multivariate normal distribution
library(mvtnorm)
bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
sigma2.curr = sigma2.init
beta.curr = beta.init
A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
# Store incomplete log-likelihoods for each iteration
log_like = c()
log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
delta.ll = 1
err = 1
n.iter = 1
while((err>epsilon)&(n.iter<=max.iter))
{
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# compute change of log-likelihoods
err <- max(abs(vtheta - vtheta_old))
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=100)
# calculating multivariate normal distribution
library(mvtnorm)
bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
sigma2.curr = sigma2.init
beta.curr = beta.init
A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
# Store incomplete log-likelihoods for each iteration
log_like = c()
log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
delta.ll = 1
err = 1
n.iter = 1
while((err>epsilon)&(n.iter<=max.iter))
{
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# compute change of log-likelihoods
err <- max(abs(vtheta - vtheta_old))
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=lam)
par(mfrow=c(2,3))
for (j in 1:p)
{
d <- density(Samples$beta[,j][-1]) # returns the density data
plot(d,main=paste0("Density of beta",j))
}
current_beta
library(glmnet)
# Choose optimal lambda
optimal_lasso_regression = function(lambda,X,y)
{
lambda = cv.glmnet(X,y,alpha=1)
lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda$lambda.min)
return(lasso_model)
}
# Choose given lambda
fixed_lasso_regression = function(lambda,X,y)
{
lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda)
return(lasso_model)
}
lasso_model = optimal_lasso_regression(lambda,X,y)
lasso_model$beta[,1]
# Set initial parameter
sample_size=100
p = 5
proportion_sparse = 0.5
sigma = 1
mean = 0
# Create Simulated y value
simulate = function(X,beta,mean,sigma,sample_size)
{
y = X %*% beta + rnorm(sample_size,mean,sigma^2)
return(y)
}
# Create a Sparse Vector for beta
createSparseVector = function(proportion_sparse,size)
{
vec = rep(0,size)
non_zero_num = proportion_sparse*size
non_zero_value = rnorm(proportion_sparse*size)
non_zero_entry = sample(1:size,non_zero_num)
vec[non_zero_entry] = non_zero_value
return(vec)
}
# Without Intercept
# Create X
X = matrix(rnorm(sample_size*p), ncol=p)
# scale X
X = scale(X)
# Create a random generated sparse vector
beta = createSparseVector(proportion_sparse,p)
# Get corresponding y
y = simulate(X,beta,mean,sigma,sample_size)
y = y - mean(y)
beta
library(glmnet)
# Choose optimal lambda
optimal_lasso_regression = function(lambda,X,y)
{
lambda = cv.glmnet(X,y,alpha=1)
lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda$lambda.min)
return(lasso_model)
}
# Choose given lambda
fixed_lasso_regression = function(lambda,X,y)
{
lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda)
return(lasso_model)
}
lasso_model = optimal_lasso_regression(lambda,X,y)
lasso_model$beta[,1]
lambda = 0.01
current_beta = c()
while(is.element(0,current_beta) == FALSE)
{
lasso_model = fixed_lasso_regression(lambda,X,y)
current_beta = lasso_model$beta[,1]
lambda = lambda * 10
}
current_beta
lambda
lasso_model$lambda
library(LaplacesDemon)
library(ghyp) # gig
library(MASS) # Ginv
# Gibbs function
gibbs.f2=function(X, y, lambda, max.steps = 10^3){
n <- nrow(X)
p <- ncol(X)
# Record sampling values
betaSamples <- matrix(0, max.steps+1, p)
sigma2Samples <- rep(0, max.steps+1)
a_j_Samples <- matrix(0, max.steps+1, p)
# Initialize
betaSamples[1,] = 0
sigma2Samples[1] = 1
a_j_Samples[1,] = 1
lambda = lambda #2
a = 1 # Prior parameter for sigma2
b = 2 # Prior parameter for sigma2
t = max.steps
for(i in 2:(t+1)) {
A = diag(a_j_Samples[i-1,])
# Sample beta
mean_beta = solve(t(X)%*%X+lambda^2*solve(A))%*%t(X)%*%y
variance_beta = solve(t(X)%*%X+lambda^2*solve(A))*sigma2Samples[i-1]
variance_beta=round(variance_beta,10)
betaSamples[i,] = rmvn(1,as.vector(mean_beta),variance_beta)
# Sample sigma2
sigma2Samples[i] = rinvgamma(1,n/2+p/2+a,(sum((y-X%*%betaSamples[i,])^2))/2 + sum(betaSamples[i,]^2/a_j_Samples[i-1,])*lambda^2/2+b)
# Sample aj
for (j in 1:p)
{
a_j_Samples[i,j] = rgig(1, 1,betaSamples[i,j]^2*lambda^2/sigma2Samples[i], 1/2)
}
}
return(list(beta = betaSamples,sigma2 = sigma2Samples,a = a_j_Samples))
}
Samples = gibbs.f2(X,y,lambda)
par(mfrow=c(2,3))
for (j in 1:p)
{
d <- density(Samples$beta[,j][-1]) # returns the density data
plot(d,main=paste0("Density of beta",j))
}
current_beta
# calculating multivariate normal distribution
library(mvtnorm)
bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
sigma2.curr = sigma2.init
beta.curr = beta.init
A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
# Store incomplete log-likelihoods for each iteration
log_like = c()
log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
delta.ll = 1
err = 1
n.iter = 1
while((err>epsilon)&(n.iter<=max.iter))
{
# Record old value of theta
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# change in value of theta
err <- max(abs(vtheta - vtheta_old))
# compute change of log-likelihoods
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=lam)
# calculating multivariate normal distribution
library(mvtnorm)
bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
sigma2.curr = sigma2.init
beta.curr = beta.init
A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
# Store incomplete log-likelihoods for each iteration
log_like = c()
log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
delta.ll = 1
err = 1
n.iter = 1
while((err>epsilon)&(n.iter<=max.iter))
{
# Record old value of theta
vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
# EM step: Replace current value with new parameter estimate
param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
beta.curr = param$beta.curr
sigma2.curr = param$sigma2.curr
A.curr = param$A.curr
vtheta <- c(beta.curr,sigma2.curr,A.curr)
# incomplete log-likelihoods with new value
log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
# change in value of theta
err <- max(abs(vtheta - vtheta_old))
# compute change of log-likelihoods
delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
# increase number of iteration
n.iter = n.iter + 1
}
print(n.iter)
print(log_like)
print(err)
return(list(beta = beta.curr,sigma2 = sigma2.curr))
}
# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{
n = dim(X)[1]
p = dim(X)[2]
ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
return(ll)
}
# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
# Estimate E[1/a_j] and build matrix
# E-step
A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
n = dim(X)[1]
p = dim(X)[2]
# Calculate new beta and new sigma2
# M-step
beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}
lam = lasso_model$lambda
bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=lam)
beta
bayesian_lasso_mfvb <- function(vy, mX, lambda, sigma2_hat)
{
MAXITER <- 500
TOL <- 1.0E-8
lambda2 <- lambda*lambda
a <- 0.0001
b <- 0.0001
n <- nrow(mX)
p <- ncol(mX)
XTX <- t(mX)%*%mX
XTy <- t(mX)%*%vy
vmu_til  <- matrix(0,p,1)
mSigma_til <- diag(1,p)
a_til <- a + 0.5*(n + p)
b_til <- sigma2_hat*a_til
vd <- rep(1,p)
vtheta <- c(vmu_til,mSigma_til,a_til, b_til)
for (ITER in 1:MAXITER)
{
# M-step
mQ <- solve(XTX + lambda2*diag(vd),tol=1.0E-99)
vmu_til    <- mQ%*%XTy
mSigma_til <- (b_til/a_til)*mQ
vsigma2_til <- diag(mSigma_til)
a_til <- a + 0.5*(n + p)
b_til <- b + 0.5*sum((vy - mX%*%vmu_til)^2) + 0.5*lambda2*sum(vd*vmu_til*vmu_til) + 0.5*sum(diag(mQ%*%mSigma_til))
# E-step
vd <- sqrt(b_til/(a_til*(vmu_til^2 + vsigma2_til)))/lambda
vd <- as.vector(vd)
vtheta_old <- vtheta
vtheta <- c(vmu_til,mSigma_til,a_til, b_til)
err <- max(abs(vtheta - vtheta_old))
cat("ITER=",ITER,"err=",err,"\n")
if (err < TOL) {
break;
}
}
return(list(vmu_til=vmu_til, mSigma_til=mSigma_til, a_til=a_til, b_til=b_til, vd=vd))
}
bayesian_lasso_mfvb(y,X,lambda = 10,sigma2_hat = 1)
