---
title: "Bayesian Lasso Gibbs Sampler"
author: "Yuhao Li"
output: html_notebook
---


# load data
```{r}
set.seed(10)
library(lars)
library("Matrix")
data("diabetes")
```

# Q1 Simulate dataset

1 simulate X with N(0,1) density with 100 sample
2 create sparse vector based on proprtion of zero, non-zero value is simulated by N(0,1)
```{r}
# Set initial parameter
sample_size=100
p = 5
proportion_sparse = 0.5
sigma = 1
mean = 0

# Create Simulated y value
simulate = function(X,beta,mean,sigma,sample_size) 
{
  y = X %*% beta + rnorm(sample_size,mean,sigma^2)
  return(y)
}
  
# Create a Sparse Vector for beta
createSparseVector = function(proportion_sparse,size)
{
  vec = rep(0,size)
  non_zero_num = proportion_sparse*size
  non_zero_value = rnorm(proportion_sparse*size)
  non_zero_entry = sample(1:size,non_zero_num)
  vec[non_zero_entry] = non_zero_value
  return(vec)
}

# Without Intercept
# Create X
X = matrix(rnorm(sample_size*p), ncol=p)
# scale X
X = scale(X)

# Create a random generated sparse vector
beta = createSparseVector(proportion_sparse,p)

# Get corresponding y
y = simulate(X,beta,mean,sigma,sample_size)
y = y - mean(y)


beta
```



## Question 2 Fit dataset with lasso regression
Optimal Lambda
```{r}
library(glmnet)

# Choose optimal lambda
optimal_lasso_regression = function(lambda,X,y)
{
  lambda = cv.glmnet(X,y,alpha=1)
  lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda$lambda.min)
  return(lasso_model)
}

# Choose given lambda
fixed_lasso_regression = function(lambda,X,y)
{
  lasso_model <- glmnet(X,y, alpha = 1, lambda = lambda)
  return(lasso_model)
}

lasso_model = optimal_lasso_regression(lambda,X,y)
lasso_model$beta[,1]
```


## Choose lambda that some of regression coefficients are fitted to be 0
## Larger lambda will have larger chance to get 0
```{r}
lambda = 0.01
current_beta = c()
while(is.element(0,current_beta) == FALSE)
{
  lasso_model = fixed_lasso_regression(lambda,X,y)
  current_beta = lasso_model$beta[,1]
  lambda = lambda * 10
}
current_beta
lambda
lasso_model$lambda

```


## Question 3 and 4 Gibbs for Bayesian Lasso

Derivation
A is diagonal matrix contain a_1,...a_p
$$
\begin{align*}
p(\beta |y,\sigma^2,a): MVN((X^TX+\lambda^2A^{-1})^{-1}X^Ty,(X^TX+\lambda^2A^{-1})^{-1}\sigma^2)\\
p(\sigma^2 |y,\beta,a_j):InverseGamma(\frac{n}{2}+\frac{p}{2}+a,\frac{||y-X\beta||_2^2}{2}+\frac{\lambda^2\sum_j{\beta_j^2}}{2a_j}+b)\\

p(a_j|y,\beta,\sigma^2): GIG(1,\frac{\beta_j^2\lambda^2}{\sigma^2},1/2)

\end{align*}
$$


Gibbs Sampler
```{r}
library(LaplacesDemon)
library(ghyp) # gig
library(MASS) # Ginv


# Gibbs function
gibbs.f2=function(X, y, lambda, max.steps = 10^3){
	n <- nrow(X)
	p <- ncol(X)


  # Record sampling values
	betaSamples <- matrix(0, max.steps+1, p)
	sigma2Samples <- rep(0, max.steps+1)
	a_j_Samples <- matrix(0, max.steps+1, p)
	
	# Initialize 
	betaSamples[1,] = 0
	sigma2Samples[1] = 1 
	a_j_Samples[1,] = 1
	
	lambda = lambda #2
	a = 1 # Prior parameter for sigma2
	b = 2 # Prior parameter for sigma2
	t = max.steps
	
	for(i in 2:(t+1)) {
	  A = diag(a_j_Samples[i-1,])
	  # Sample beta
	  mean_beta = solve(t(X)%*%X+lambda^2*solve(A))%*%t(X)%*%y
	  variance_beta = solve(t(X)%*%X+lambda^2*solve(A))*sigma2Samples[i-1]
	  variance_beta=round(variance_beta,10)
	  betaSamples[i,] = rmvn(1,as.vector(mean_beta),variance_beta)
	  
	  # Sample sigma2
	  sigma2Samples[i] = rinvgamma(1,n/2+p/2+a,(sum((y-X%*%betaSamples[i,])^2))/2 + sum(betaSamples[i,]^2/a_j_Samples[i-1,])*lambda^2/2+b)
	  
	  # Sample aj
	  for (j in 1:p)
	  {
	    a_j_Samples[i,j] = rgig(1, 1,betaSamples[i,j]^2*lambda^2/sigma2Samples[i], 1/2)
	  }
	}
	
	return(list(beta = betaSamples,sigma2 = sigma2Samples,a = a_j_Samples))
}
Samples = gibbs.f2(X,y,lambda)

```

Question 5 Plot Denstiy Plots for each $\beta_j$
```{r}
par(mfrow=c(2,3))
for (j in 1:p)
{
	 d <- density(Samples$beta[,j][-1]) # returns the density data
   plot(d,main=paste0("Density of beta",j))
}

current_beta
```



# Bayesian Lasso Model EM 
# E-step derivation
$$ 
Q(\theta,\theta^{(t)}) = -(\frac{n}{2}+\frac{p}{2}+a+1)log(\sigma^2)-\frac{b}{\sigma^2}-\frac{||y-X\beta||_2^2}{2\sigma^2}-\frac{\lambda^2}{2\sigma^2}\sum_{j=1}^{p}(\beta_j^2 E[\frac{1}{a_j}])\\
E[\frac{1}{a_j}] = \frac{\sigma^{(t)}}{|\beta_j^{(t)}|\lambda}
$$
# M-step
$$ 
\frac{\partial Q}{\partial\beta}=0 \to \hat{\beta} = (X^TX+\lambda^2\hat{A})^{-1}X^Ty\\
\hat{A}_{jj} = \begin{bmatrix}
          \frac{\hat{\sigma}}{|\beta_j|\lambda}
          \end{bmatrix}\\
\frac{\partial Q}{\partial\sigma^2}=0\to \hat{\sigma^2}=\frac{\frac{||y-X\beta||_2^2+\lambda^2\beta^TA\beta}{2}+b}{\frac{n+p}{2}+a+1}
$$


```{r}
# calculating multivariate normal distribution
library(mvtnorm)

bayesianLassoEM = function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
  sigma2.curr = sigma2.init
  beta.curr = beta.init
  A.curr = diag(as.vector(sqrt(sigma2.curr)/(lambda*as.vector(abs(beta.curr)+1.0e-8))))
  
  # Store incomplete log-likelihoods for each iteration
  log_like = c()
  log_like = c(log_like,compute_ll(X,y,beta.init,sigma2.init,lambda,a,b,A.curr))
  delta.ll = 1
  err = 1
  n.iter = 1
  
  while((err>epsilon)&(n.iter<=max.iter))
  {
    # Record old value of theta
    vtheta_old <- c(beta.curr,sigma2.curr,A.curr)
    # EM step: Replace current value with new parameter estimate
    param = EM_step(X,y,beta.curr,sigma2.curr,lambda,a,b)
    beta.curr = param$beta.curr
    sigma2.curr = param$sigma2.curr
    A.curr = param$A.curr
    vtheta <- c(beta.curr,sigma2.curr,A.curr)
    # incomplete log-likelihoods with new value
    log_like = c(log_like,compute_ll(X,y,beta.curr,sigma2.curr,lambda,a,b,A.curr))
    # change in value of theta
    err <- max(abs(vtheta - vtheta_old))
    # compute change of log-likelihoods
    delta.ll = log_like[length(log_like)]-log_like[length(log_like)-1]
    # increase number of iteration
    n.iter = n.iter + 1
    
  }
  
  print(n.iter)
  print(log_like)
  print(err)
  return(list(beta = beta.curr,sigma2 = sigma2.curr))
}

# compute incomplete log likelihood value
compute_ll = function(X,y,beta,sigma2,lambda,a,b,A)
{

  n = dim(X)[1]
  p = dim(X)[2]
  ll = -(n/2+p/2+a+1)*log(sigma2) - norm(y-X%*%beta,'2')^2/(2*sigma2)-lambda^2/(2*sigma2)*(t(beta)%*%A%*%beta)-b/sigma2
  return(ll)
}

# E-step and M-step
EM_step = function(X,y,beta,sigma2,lambda,a,b)
{
  # Estimate E[1/a_j] and build matrix
  # E-step
  A.curr = diag(sqrt(as.vector(sigma2))/(lambda*as.vector(abs(beta)+1.0e-8)))
  n = dim(X)[1]
  p = dim(X)[2]
  
  # Calculate new beta and new sigma2
  # M-step
  beta.curr = solve(t(X)%*%X+lambda^2*A.curr)%*%(t(X)%*%y)
  sigma2.curr = (norm(y-X%*%beta,'2')^2/2+lambda^2*(t(beta)%*%A.curr%*%beta)/2+b)/(n/2+p/2+a+1)
  return(list(beta.curr = beta.curr,sigma2.curr = sigma2.curr,A.curr = A.curr))
}

lam = lasso_model$lambda
output = bayesianLassoEM(X,y,beta.init = rep(1,p),sigma2.init=1,epsilon = 1e-3,max.iter = 100,a=1,b=2,lambda=lam)

beta

```

# Variational bayes
Assume optimal densities are from mean field families
$$
q(\beta,\sigma^2,a) = q(\beta)q(\sigma^2)q(a)
$$
where
$$
q_{\beta^*}(\beta) \propto exp[E_{a,\sigma^2}\{log(p(\beta|\sigma^2,a)\}]\\
q_{\sigma^{2*}}(\sigma^2) \propto exp[E_{\beta,a}\{log(p(\sigma^2|\beta,a)\}]\\
q_{a^*}(a) \propto exp[E_{\sigma^2,\beta}\{log(p(a|\sigma^2,\beta)\}]\\
$$

$$
q_{\beta^*}(\beta): MVN(\mu^*,\Sigma^{*})), \mu^* = (X^TX+\lambda^2 E_a(A))^{-1}X^Ty,\Sigma^{*}= (X^TX+\lambda^2 E_a(A))^{-1}E_{\sigma^2}(\sigma^2)\\
q_{\sigma^{2*}}(\sigma^2): InverseGamma(a_{\sigma^2}^*,b_{\sigma^2}
^*), a_{\sigma^2}^*=\frac{n}{2}+\frac{p}{2}+a,b_{\sigma^2}^*=\frac{||y-XE_\beta(\beta)||_2^2}{2}+\lambda^2\sum_j{E_\beta(\beta_j^2)}{E[\frac{1}{a_j}]}+b\\

q_{a^*}(a):GIG(a_{a_j}^*,b_{a_j}^*,p_{a_j}^*), a_{a_j}^* = 1, b_{a_j^*}= E_{\beta}[\beta_j^2]E_{\sigma^2}[\frac{1}{\sigma^2}]\lambda^2 
,p_{a_j^*} =1/2\\

E_a(A) = diag(E[\frac{1}{a_j}]),
E[\frac{1}{a_j}] = \frac{\sigma^{(t)}}{|\beta_j^{(t)}|\lambda}\\
E_{\sigma^2}(\sigma^2) = \frac{b^*}{a^*-1}\\
E_\beta[\beta] = \mu^*\\
E_{\sigma^2}[\frac{1}{\sigma^2}] = \frac{a^*}{b^*} \\
E_\beta(\beta_j^2)= \mu_j^{*2} + \Sigma^*_{jj}
$$


```{r}
bayesian_lasso_mfvb <- function(vy, mX, lambda, sigma2_hat)
{
  MAXITER <- 500
  TOL <- 1.0E-8
  lambda2 <- lambda*lambda
  a <- 0.0001
  b <- 0.0001
  n <- nrow(mX)
  p <- ncol(mX)
  XTX <- t(mX)%*%mX
  XTy <- t(mX)%*%vy
  vmu_til  <- matrix(0,p,1)
  mSigma_til <- diag(1,p)
  a_til <- a + 0.5*(n + p)
  b_til <- sigma2_hat*a_til
  vd <- rep(1,p)
  vtheta <- c(vmu_til,mSigma_til,a_til, b_til)
  
  for (ITER in 1:MAXITER)
  {
    # M-step
    mQ <- solve(XTX + lambda2*diag(vd),tol=1.0E-99)
    vmu_til    <- mQ%*%XTy
    mSigma_til <- (b_til/a_til)*mQ
    vsigma2_til <- diag(mSigma_til)
    a_til <- a + 0.5*(n + p)
    b_til <- b + 0.5*sum((vy - mX%*%vmu_til)^2) + 0.5*lambda2*sum(vd*vmu_til*vmu_til) + 0.5*sum(diag(mQ%*%mSigma_til))
    
    # E-step
    vd <- sqrt(b_til/(a_til*(vmu_til^2 + vsigma2_til)))/lambda
    vd <- as.vector(vd)
    vtheta_old <- vtheta
    vtheta <- c(vmu_til,mSigma_til,a_til, b_til)
    err <- max(abs(vtheta - vtheta_old))
    cat("ITER=",ITER,"err=",err,"\n")
    if (err < TOL) {
      break;
    }
  }
  return(list(vmu_til=vmu_til, mSigma_til=mSigma_til, a_til=a_til, b_til=b_til, vd=vd))
}

params = bayesian_lasso_mfvb(y,X,lambda = 0.1,sigma2_hat = 1)
params
```

Using MFVB to get
mu_til: initial global mean
Sigma_til: initial global covariance

Continued Fraction: Lenz's algorithm in C
Log trick

# Local and Global algorithm 
UTILS
```{r}

###################  UNIVARIATE LASSO DISTRIBUTION ###############################

lasso_f <- function(x,a,b,c)
{
  return(x^2*exp(-0.5*a*x^2+b*x-c*abs(x)))
}
# Note: a>0,c>0,b in R
calculate_uni_lasso_param <- function(a,b,c)
{
  mu1 = (b-c)/a
  mu2 = -(c+b)/a
  sigma2 = 1/a
  Z = norm_const_lasso(mu1,mu2,sqrt(sigma2))
  mu = mean_uni_lasso(mu1,mu2,sqrt(sigma2),Z)
  var = var_uni_lasso(mu1,mu2,sqrt(sigma2),Z)
  return(list(mu = mu,var = var, Z = Z))
}

# Calculate normalizing constant
norm_const_lasso <- function(mu1,mu2,sigma)
{
  # Log trick
  Z = sigma*(pnorm(mu1/sigma)/dnorm(mu1/sigma) + pnorm(mu2/sigma)/dnorm(mu2/sigma))
  return(Z)
}

# Calculate expectation of lasso distribution
mean_uni_lasso <- function(mu1,mu2,sigma,Z)
{
  part1 = exp(pnorm(mu1/sigma,log.p = TRUE)- dnorm(mu1/sigma,log = TRUE))*pos_tru_exp(mu1,sigma)
  part2 = exp(pnorm(mu2/sigma,log.p = TRUE)- dnorm(mu2/sigma,log = TRUE))*pos_tru_exp(mu2,sigma)
  mean = sigma/Z *(part1 - part2 )
  return(mean)
}

# Calculate second moment of lasso distribution
second_mom_uni_lasso = function(mu1,mu2,sigma,Z)
{
  part1 = exp(pnorm(mu1/sigma,log.p = TRUE)- dnorm(mu1/sigma,log = TRUE))*pos_tru_sec_mom(mu1,sigma)
  part2 = exp(pnorm(mu2/sigma,log.p = TRUE)- dnorm(mu2/sigma,log = TRUE))*pos_tru_sec_mom(mu2,sigma)
  
  sec_mom = sigma/Z *(part1 + part2 ) 
  return(sec_mom)
}

# Calculate variance of lasso distribution
var_uni_lasso <- function(mu1,mu2,sigma,Z)
{
  var = second_mom_uni_lasso(mu1,mu2,sigma,Z) -(mean_uni_lasso(mu1,mu2,sigma,Z))^2
  return(var)
}


#################################  Positive Truncated Norm ##################################

# Calculate expectation of truncated normal distribution
pos_tru_exp <- function(mu,sigma)
{
  return(mu + sigma*xsi_1(mu/sigma))
}

# Calculate second moment of truncated normal distribution
pos_tru_sec_mom <- function(mu,sigma)
{
  return(sigma^2*(1+xsi_2(mu/sigma)) +(mu + sigma * xsi_1(mu/sigma))^2)
}
  
# xsi 1 function
xsi_1<- function(t)
{
  
  return(exp(dnorm(t,log = TRUE)- pnorm(t,log.p = TRUE)))
}

# xsi 2 function
xsi_2<- function(t)
{
  return(-t*xsi_1(t) - xsi_1(t)^2)
}


```
# Integrate function in R to check

```{r}
calculate_uni_lasso_param(2,3,4)
```
### Using Integrate function to test
```{r}
f = function(x)
{ 
  a=2
  b=3
  c=4
  return(exp(-0.5*a*x^2+b*x-c*abs(x)))
}

expect = function(x)
{ 
  a=2
  b=3
  c=4
  return(x*exp(-0.5*a*x^2+b*x-c*abs(x)))
}

sec_mom = function(x)
{ 
  a=2
  b=3
  c=4
  return(x^2*exp(-0.5*a*x^2+b*x-c*abs(x)))
}
Z = integrate(f,-10,10)$value
exp = integrate(expect,-10,10)$value / Z
sec_mom = integrate(sec_mom,-10,10)$value / Z
var = sec_mom - exp^2

Z
exp
var
```


Multivariate Lasso Distribution
```{r}
###################  MULTIVARIATE LASSO DISTRIBUTION ###############################

# Note: mA: positive definite,c vb in R2, cc>0, 
calculate_multi_lasso_param <- function(mA,vb,cc)
{
  A_inv = solve(mA)
  mu1 = A_inv %*%(b - c)
  mu2 = A_inv %*% matrix(c(b[1]-c,-b[2]-c),2,1)
  mu3 = A_inv %*% matrix(c(-b[1]-c,b[2]-c),2,1)
  mu4 = A_inv %*%(b + c)
  Sigma = A_inv
  Z = multi_norm_const_lasso(mu1,mu2,mu3,mu4,Sigma)
  mu = mean_multi_lasso(mu1,mu2,mu3,mu4,Sigma,Z)
  var = var_multi_lasso(mu1,mu2,mu3,mu4,Sigma,Z)
  return(list(mu = mu,var = var, Z = Z))
}
```


## Verify multivariate lasso

```{r}
library(pracma)
fun <- function(x,y) 
{
  A = matrix(c(2,0,0,3),2,2)
  b = matrix(c(-2,1),2,1)
  c = 3
  vx = matrix(c(x,y),2,1)
  return(exp((-0.5*t(vx)%*%A%*%vx + t(b)%*%vx-  c*norm(vx,"1"))[1]))
}
xmin <- -10; xmax <- 10
ymin <- -10; ymax <- 10

# Normalizing constant
integral2(fun, xmin, xmax, ymin, ymax)

```




Local and Global Algorithm
## Update one variable at a time (expected fast but less accurate result)
# vy: vector of y variable
# mX: Design Matrix
# Beta: estimated posterior regression coefficient
# lambda: penalized parameter
# Params: Parameter output from MFVB

```{r}
## INPUT: y, mX, lambda, param
## OUTPUT: Global adjust mean, Global adjust covariance

## If there is a scalar output, change it to 1*1 matrix output, otherwise remain same


local_global_algorithm_1 <- function(vy, mX, lambda, params)
{
  ## Initialization
  MAXITER <- 500
  TOL <- 1.0E-7
  n <- nrow(mX)
  p <- ncol(mX)
  
  ## Initial Global Mean and Global Covariance From MFVB
  vmu_adjust = params$vmu_til
  mSigma_adjust = params$mSigma_til
  
  # Later will be changed
  a_til = params$a_til
  b_til = params$b_til
  
  # Record the local parameter
  va = c()
  vb = c()
  vZ = c()
  
  # Initial value Local parameters
  vtheta <- c(vmu_adjust)
  
  for (ITER in 1:MAXITER)
  {
    ## Local Update
    for (j in 1:p)
    {
      # Local Update
      vs = matrix(vmu_adjust[-j]) - matrix(mSigma_adjust[-j,j]) %*% matrix(solve(mSigma_adjust)[j,j]) %*% matrix(vmu_adjust[j])
      mt = matrix(mSigma_adjust[-j,j]) %*% matrix(solve(mSigma_adjust)[j,j])
      
      ## Update local parameter
      a = a_til/b_til*(t(mX[,j]) %*% mX[,j] + t(mX[,j])%*%mX[,-j]%*%mt)
      b = a_til/b_til*t(mX[,j])%*%(vy-mX[,-j]%*%vs)
      c = lambda * gamma(a_til+0.5)/(gamma(a_til)*sqrt(b_til))
      
      ## Calculate Local mean and variance
      vlocal_mean = calculate_uni_lasso_param(a,b,c)$mu
      mlocal_var = calculate_uni_lasso_param(a,b,c)$var
      
      # Record local parameter distribution
      va[j] = a
      vb[j] = b
      vZ[j] = calculate_uni_lasso_param(a,b,c)$Z
      
      # Global Update
      ## Update Mean
      vmu_adjust[j] = vlocal_mean
      vmu_adjust[-j] = matrix(vmu_adjust[-j]) + matrix(mSigma_adjust[-j,j]) %*% matrix(solve(mSigma_adjust)[j,j]) %*% matrix(vlocal_mean - vmu_adjust[j])
      
      ##  Update Covariance
      j_len = length(vlocal_mean)
      mSigma_jj_inv = matrix(solve(mSigma_adjust)[j,j],j_len,j_len)
      
      mSigma_adjust[j,j] = mlocal_var
      mSigma_adjust[j,-j] = mlocal_var %*% mSigma_jj_inv %*% t(matrix(mSigma_adjust[j,-j]))
      mSigma_adjust[-j,j] = t(mSigma_adjust[j,-j])
      mSigma_adjust[-j,-j] = matrix(mSigma_adjust[-j,-j],p-j_len,p-j_len) + matrix(mSigma_adjust[-j,j]) %*% mSigma_jj_inv %*% (mlocal_var - matrix(mSigma_adjust[j,j]))  %*%mSigma_jj_inv %*% t(matrix(mSigma_adjust[j,-j]))
      
    }

    # Update Theta
    vtheta_old <- vtheta
    vtheta <- c(vmu_adjust)
    
    ## Check stopping criterion    
    err <- max(abs(vtheta - vtheta_old))
    cat("ITER=",ITER,"err=",err,"\n")
    if (err < TOL) {
      break;
    }
  }
  # Return Local and Global parameter
  return(list("mu" = vmu_adjust,"Sigma" = mSigma_adjust, "a" = va,"b" = vb,"c"= c ,"Z" = vZ))
}

output1 = local_global_algorithm_1(y,X,lambda = 0.1,params)
output1
```
Compare density, MCMC, local density and global density
```{r}
# Initialization
va = output1$a
vb = output1$b
cc = output1$c
vZ = output1$Z
global_mu = output1$mu
global_Sigma = output1$Sigma

# Drawing
par(mfrow=c(2,3))
for (j in 1:p)
{
   # returns the density of MCMC samples
	 mcmc = density(Samples$beta[,j][-1], from = -2, to = 2)
	 plot(mcmc,main=paste0("Density of beta",j),col="blue")

	 # draw local parameter distribution
	 a = va[j]
	 b = vb[j]
	 c = cc
	 Z = vZ[j]
	 lasso = curve(exp(-0.5*a*x^2+b*x-c*abs(x))/Z,add = TRUE,from = - 2, to = 2,col="green")
	 

	 # # draw global parameter distribution
	 global = curve(dnorm(x, mean = global_mu[j],sd = global_Sigma[j,j]), add = TRUE,from = -2, to = 2,col="red")

   
}
```




## Update two variable at a time(Expected more accurate but less speed)
## Update for each pair of variables
```{r}
local_global_algorithm_2 <- function(vy, mX, lambda, params)
{
  
  
  
  
}
```



## Focus on non-zero element of parameter only (Faster speed, faster accuracy)
```{r}
local_global_algorithm_3 <- function(X,y,beta.init,sigma2.init,epsilon = 1e-3,max.iter = 100,a,b,lambda)
{
  
  
}
```




